package proxy

import (
	"context"
	"fmt"
	"log/slog"
	"math"
	"net/http"
	"strings"
	"sync"
	"time"

	"cc-forwarder/config"
	"cc-forwarder/internal/endpoint"
	"cc-forwarder/internal/tracking"
)

// RetryHandler handles retry logic with exponential backoff
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ internal/proxy/retry.RetryController æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
//
// æ–°çš„é‡è¯•æ¶æ„æä¾›äº†ä»¥ä¸‹ä¼˜åŠ¿ï¼š
// - ç»Ÿä¸€çš„é‡è¯•ç­–ç•¥ï¼ˆå¸¸è§„å’Œæµå¼è¯·æ±‚ä½¿ç”¨ç›¸åŒç®—æ³•ï¼‰
// - æ›´å¥½çš„é”™è¯¯åˆ†ç±»å’Œå†³ç­–é€»è¾‘
// - æ”¯æŒé™æµé”™è¯¯çš„ç‰¹æ®Šå¤„ç†
// - æ›´æ¸…æ™°çš„ä»£ç ç»“æ„å’Œå¯æµ‹è¯•æ€§
type RetryHandler struct {
	config          *config.Config
	endpointManager *endpoint.Manager
	monitoringMiddleware interface{
		RecordRetry(connID string, endpoint string)
	}
	usageTracker    *tracking.UsageTracker
	
	// Request suspension related fields
	suspendedRequestsMutex sync.RWMutex
	suspendedRequestsCount int
}

// NewRetryHandler creates a new retry handler
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ internal/proxy/retry.NewRetryController æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func NewRetryHandler(cfg *config.Config) *RetryHandler {
	return &RetryHandler{
		config: cfg,
	}
}

// SetEndpointManager sets the endpoint manager
func (rh *RetryHandler) SetEndpointManager(manager *endpoint.Manager) {
	rh.endpointManager = manager
}

// SetMonitoringMiddleware sets the monitoring middleware
func (rh *RetryHandler) SetMonitoringMiddleware(mm interface{
	RecordRetry(connID string, endpoint string)
}) {
	rh.monitoringMiddleware = mm
}

// SetUsageTracker sets the usage tracker
func (rh *RetryHandler) SetUsageTracker(ut *tracking.UsageTracker) {
	rh.usageTracker = ut
}

// Operation represents a function that can be retried, returns response and error
type Operation func(ep *endpoint.Endpoint, connID string) (*http.Response, error)

// RetryableError represents an error that can be retried with additional context
type RetryableError struct {
	Err        error
	StatusCode int
	IsRetryable bool
	Reason     string
}

func (re *RetryableError) Error() string {
	if re.Err != nil {
		return re.Err.Error()
	}
	return fmt.Sprintf("HTTP %d", re.StatusCode)
}

// Execute executes an operation with retry and fallback logic
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ RetryController.ExecuteWithRetry æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func (rh *RetryHandler) Execute(operation Operation, connID string) (*http.Response, error) {
	return rh.ExecuteWithContext(context.Background(), operation, connID)
}

// ExecuteWithContext executes an operation with context, retry and fallback logic with dynamic group management
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ RetryController.ExecuteWithRetry æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func (rh *RetryHandler) ExecuteWithContext(ctx context.Context, operation Operation, connID string) (*http.Response, error) {
	var lastErr error
	var lastResp *http.Response
	var totalEndpointsAttempted int
	
	// Track groups that have been put into cooldown during this request
	groupsSetToCooldownThisRequest := make(map[string]bool)
	
	for {
		// Get healthy endpoints from currently active groups only (no auto group switching)
		var endpoints []*endpoint.Endpoint
		if rh.endpointManager.GetConfig().Strategy.Type == "fastest" && rh.endpointManager.GetConfig().Strategy.FastTestEnabled {
			endpoints = rh.endpointManager.GetFastestEndpointsWithRealTimeTest(ctx)
		} else {
			endpoints = rh.endpointManager.GetHealthyEndpoints()
		}
		
		// If no endpoints available from active groups, check if we should suspend request
		if len(endpoints) == 0 {
			// æ£€æŸ¥æ˜¯å¦åº”è¯¥æŒ‚èµ·è¯·æ±‚
			if rh.shouldSuspendRequest(ctx) {
				slog.InfoContext(ctx, fmt.Sprintf("ğŸ”„ [å°è¯•æŒ‚èµ·] è¿æ¥ %s å½“å‰æ´»è·ƒç»„æ— å¯ç”¨ç«¯ç‚¹ï¼Œå°è¯•æŒ‚èµ·è¯·æ±‚ç­‰å¾…ç»„åˆ‡æ¢", connID))
				
				// æŒ‚èµ·è¯·æ±‚ç­‰å¾…ç»„åˆ‡æ¢
				if rh.waitForGroupSwitch(ctx, connID) {
					slog.InfoContext(ctx, fmt.Sprintf("ğŸš€ [æŒ‚èµ·æ¢å¤] è¿æ¥ %s ç»„åˆ‡æ¢å®Œæˆï¼Œé‡æ–°è¿›å…¥é‡è¯•å¾ªç¯", connID))
					// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
					// å†å²æ³¨é‡Šï¼šæ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºè½¬å‘ä¸­ï¼ˆä»æŒ‚èµ·çŠ¶æ€æ¢å¤ï¼‰
					continue // é‡æ–°è¿›å…¥å¤–å±‚å¾ªç¯ï¼Œè·å–æ–°çš„ç«¯ç‚¹åˆ—è¡¨
				} else {
					slog.WarnContext(ctx, fmt.Sprintf("âš ï¸ [æŒ‚èµ·å¤±è´¥] è¿æ¥ %s æŒ‚èµ·ç­‰å¾…è¶…æ—¶æˆ–è¢«å–æ¶ˆï¼Œç»§ç»­åŸæœ‰é”™è¯¯å¤„ç†", connID))
					// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
					// å†å²æ³¨é‡Šï¼šæ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºè¶…æ—¶ï¼ˆæŒ‚èµ·å¤±è´¥ï¼‰
					// ç»§ç»­æ‰§è¡ŒåŸæœ‰çš„é”™è¯¯å¤„ç†é€»è¾‘
				}
			}
			
			slog.WarnContext(ctx, "âš ï¸ [æ— å¯ç”¨ç«¯ç‚¹] å½“å‰æ´»è·ƒç»„ä¸­æ²¡æœ‰å¥åº·çš„ç«¯ç‚¹ï¼Œéœ€è¦æ‰‹åŠ¨åˆ‡æ¢åˆ°å…¶ä»–ç»„")
			break
		}

		// Group endpoints by group name for failure tracking
		groupEndpoints := make(map[string][]*endpoint.Endpoint)
		for _, ep := range endpoints {
			groupName := ep.Config.Group
			if groupName == "" {
				groupName = "Default"
			}
			groupEndpoints[groupName] = append(groupEndpoints[groupName], ep)
		}
		
		// Track which groups failed completely in this iteration
		groupsFailedThisIteration := make(map[string]bool)
		endpointsTriedThisIteration := 0
		
		// Try each endpoint in current endpoint set
		for endpointIndex, ep := range endpoints {
			totalEndpointsAttempted++
			endpointsTriedThisIteration++
			
			// Add endpoint info to context for logging
			ctxWithEndpoint := context.WithValue(ctx, "selected_endpoint", ep.Config.Name)
			
			groupName := ep.Config.Group
			if groupName == "" {
				groupName = "Default"
			}
			
			slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("ğŸ¯ [è¯·æ±‚è½¬å‘] [%s] é€‰æ‹©ç«¯ç‚¹: %s (ç»„: %s, æ€»å°è¯• %d)", 
				connID, ep.Config.Name, groupName, totalEndpointsAttempted))
			
			// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
			// å†å²æ³¨é‡Šï¼šRecord endpoint selection in usage tracking
			
			// Retry logic for current endpoint
			for attempt := 1; attempt <= rh.config.Retry.MaxAttempts; attempt++ {
				select {
				case <-ctx.Done():
					if lastResp != nil {
						lastResp.Body.Close()
					}
					// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
					// å†å²æ³¨é‡Šï¼šè®°å½•è¯·æ±‚å–æ¶ˆçŠ¶æ€
					return nil, ctx.Err()
				default:
				}

				// Execute operation
				resp, err := operation(ep, connID)
				if err == nil && resp != nil {
					// Check if response status code indicates success or should be retried
					retryDecision := rh.shouldRetryStatusCode(resp.StatusCode)
					
					if !retryDecision.IsRetryable {
						// åŒºåˆ†çœŸæ­£çš„æˆåŠŸå’Œä¸å¯é‡è¯•çš„é”™è¯¯
						if resp.StatusCode >= 200 && resp.StatusCode < 400 {
							// 2xx/3xx - çœŸæ­£çš„æˆåŠŸ
							slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("âœ… [è¯·æ±‚æˆåŠŸ] [%s] ç«¯ç‚¹: %s (ç»„: %s), çŠ¶æ€ç : %d (æ€»å°è¯• %d ä¸ªç«¯ç‚¹)",
								connID, ep.Config.Name, groupName, resp.StatusCode, totalEndpointsAttempted))

							// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
							// å†å²æ³¨é‡Šï¼šRecord success in usage tracking
						} else {
							// 4xx/5xx - ä¸å¯é‡è¯•çš„é”™è¯¯ï¼ˆå¦‚404, 401ç­‰ï¼‰
							slog.ErrorContext(ctxWithEndpoint, fmt.Sprintf("âŒ [è¯·æ±‚å¤±è´¥] [%s] ç«¯ç‚¹: %s (ç»„: %s), çŠ¶æ€ç : %d - %s (æ€»å°è¯• %d ä¸ªç«¯ç‚¹)", 
								connID, ep.Config.Name, groupName, resp.StatusCode, retryDecision.Reason, totalEndpointsAttempted))

							// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
							// å†å²æ³¨é‡Šï¼šRecord error in usage tracking
						}
						
						return resp, nil
					}
					
					// Status code indicates we should retry
					slog.WarnContext(ctxWithEndpoint, fmt.Sprintf("ğŸ”„ [éœ€è¦é‡è¯•] [%s] ç«¯ç‚¹: %s (ç»„: %s, å°è¯• %d/%d) - çŠ¶æ€ç : %d (%s)", 
						connID, ep.Config.Name, groupName, attempt, rh.config.Retry.MaxAttempts, resp.StatusCode, retryDecision.Reason))
					
					// Close the response body before retrying
					resp.Body.Close()
					lastErr = &RetryableError{
						StatusCode: resp.StatusCode,
						IsRetryable: true,
						Reason: retryDecision.Reason,
					}
				} else {
					// Network error or other failure
					lastErr = err
					if err != nil {
						// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œç±»å‹åˆ¤æ–­ä¸å†éœ€è¦
						// å†å²æ³¨é‡Šï¼šç¡®å®šé”™è¯¯çŠ¶æ€ç±»å‹
						
						slog.WarnContext(ctxWithEndpoint, fmt.Sprintf("âŒ [ç½‘ç»œé”™è¯¯] [%s] ç«¯ç‚¹: %s (ç»„: %s, å°è¯• %d/%d) - é”™è¯¯: %s", 
							connID, ep.Config.Name, groupName, attempt, rh.config.Retry.MaxAttempts, err.Error()))
						
						// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
						// å†å²æ³¨é‡Šï¼šRecord error with proper status in usage tracking
					}
				}

				// Don't wait after the last attempt on the current endpoint
				if attempt == rh.config.Retry.MaxAttempts {
					break
				}

				// Record retry (we're about to retry)
				if rh.monitoringMiddleware != nil && connID != "" {
					rh.monitoringMiddleware.RecordRetry(connID, ep.Config.Name)
				}
				
				// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
				// å†å²æ³¨é‡Šï¼šæ›´æ–°çŠ¶æ€ä¸ºretryï¼ˆåŒç«¯ç‚¹é‡è¯•ä¹Ÿæ˜¯é‡è¯•çŠ¶æ€ï¼‰

				// Calculate delay with exponential backoff
				delay := rh.calculateDelay(attempt)
				
				slog.InfoContext(ctxWithEndpoint, fmt.Sprintf("â³ [ç­‰å¾…é‡è¯•] [%s] ç«¯ç‚¹: %s (ç»„: %s) - %såè¿›è¡Œç¬¬%dæ¬¡å°è¯•", 
					connID, ep.Config.Name, groupName, delay.String(), attempt+1))

				// Wait before retry
				select {
				case <-ctx.Done():
					if lastResp != nil {
						lastResp.Body.Close()
					}
					// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
					// å†å²æ³¨é‡Šï¼šè®°å½•è¯·æ±‚å–æ¶ˆçŠ¶æ€
					return nil, ctx.Err()
				case <-time.After(delay):
					// Continue to next attempt
				}
			}

			slog.ErrorContext(ctxWithEndpoint, fmt.Sprintf("ğŸ’¥ [ç«¯ç‚¹å¤±è´¥] [%s] ç«¯ç‚¹ %s (ç»„: %s) æ‰€æœ‰ %d æ¬¡å°è¯•å‡å¤±è´¥", 
				connID, ep.Config.Name, groupName, rh.config.Retry.MaxAttempts))

			// Check if all endpoints in this group have been tried and failed in this iteration
			groupEndpointsCount := len(groupEndpoints[groupName])
			failedEndpointsInGroup := 0
			for _, groupEp := range groupEndpoints[groupName] {
				// Count endpoints in this group that we've already tried in this iteration
				for i := 0; i <= endpointIndex; i++ {
					if endpoints[i].Config.Name == groupEp.Config.Name {
						failedEndpointsInGroup++
						break
					}
				}
			}
			
			// If all endpoints in current group have failed in this iteration, mark group as failed
			if failedEndpointsInGroup == groupEndpointsCount {
				groupsFailedThisIteration[groupName] = true
			}
		}
		
		// After trying all endpoints in current iteration, put failed groups into cooldown
		for groupName := range groupsFailedThisIteration {
			if !groupsSetToCooldownThisRequest[groupName] {
				slog.WarnContext(ctx, fmt.Sprintf("â„ï¸ [ç»„å¤±è´¥] ç»„ %s ä¸­æ‰€æœ‰ç«¯ç‚¹å‡å·²å¤±è´¥ï¼Œå°†ç»„è®¾ç½®ä¸ºå†·å´çŠ¶æ€", groupName))
				rh.endpointManager.GetGroupManager().SetGroupCooldown(groupName)
				groupsSetToCooldownThisRequest[groupName] = true
			}
		}
		
		// Check if automatic switching between groups is enabled
		if rh.endpointManager.GetConfig().Group.AutoSwitchBetweenGroups {
			// Auto mode: Check if there are still active groups available after cooldown
			// Get fresh endpoint list to see if any new groups became active
			var newEndpoints []*endpoint.Endpoint
			if rh.endpointManager.GetConfig().Strategy.Type == "fastest" && rh.endpointManager.GetConfig().Strategy.FastTestEnabled {
				newEndpoints = rh.endpointManager.GetFastestEndpointsWithRealTimeTest(ctx)
			} else {
				newEndpoints = rh.endpointManager.GetHealthyEndpoints()
			}
			
			// If we have new endpoints available (from different groups), continue the retry loop
			if len(newEndpoints) > 0 && len(groupsFailedThisIteration) > 0 {
				// Check if the new endpoints are from different groups than what we just tried
				newGroupsAvailable := false
				newGroups := make(map[string]bool)
				for _, ep := range newEndpoints {
					groupName := ep.Config.Group
					if groupName == "" {
						groupName = "Default"
					}
					newGroups[groupName] = true
				}
				
				// Check if any new group is available that wasn't in the failed iteration
				for newGroup := range newGroups {
					if !groupsFailedThisIteration[newGroup] {
						newGroupsAvailable = true
						break
					}
				}
				
				if newGroupsAvailable {
					slog.InfoContext(ctx, fmt.Sprintf("ğŸ”„ [è‡ªåŠ¨ç»„åˆ‡æ¢] æ£€æµ‹åˆ°æ–°çš„æ´»è·ƒç»„ï¼Œç»§ç»­é‡è¯• (å·²å°è¯• %d ä¸ªç«¯ç‚¹)", totalEndpointsAttempted))
					continue // Continue outer loop with fresh endpoint list
				}
			}
		} else {
			// Manual mode: Check if any groups failed this iteration for manual intervention alert
			if len(groupsFailedThisIteration) > 0 {
				failedGroupNames := make([]string, 0, len(groupsFailedThisIteration))
				for groupName := range groupsFailedThisIteration {
					failedGroupNames = append(failedGroupNames, groupName)
				}
				slog.WarnContext(ctx, fmt.Sprintf("âš ï¸ [éœ€è¦æ‰‹åŠ¨å¹²é¢„] ç»„å¤±è´¥éœ€è¦æ‰‹åŠ¨åˆ‡æ¢ï¼Œå¤±è´¥çš„ç»„: %v - è¯·é€šè¿‡Webç•Œé¢é€‰æ‹©å…¶ä»–å¯ç”¨ç»„", failedGroupNames))
			}
			// In manual mode, continue the outer loop to check if requests should be suspended
			// The outer loop will detect len(endpoints) == 0 and trigger suspension logic if enabled
			slog.InfoContext(ctx, "ğŸ”„ [æ‰‹åŠ¨æ¨¡å¼] ç»§ç»­å¤–å±‚å¾ªç¯æ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‚èµ·è¯·æ±‚")
			continue
		}
		
		// Auto mode: No more endpoints in current active group, stop retry loop
		break
	}

		// Check if automatic switching is enabled and provide appropriate error message
	if rh.endpointManager.GetConfig().Group.AutoSwitchBetweenGroups {
		// Auto mode error message
		slog.ErrorContext(ctx, fmt.Sprintf("ğŸ’¥ [å…¨éƒ¨å¤±è´¥] æ‰€æœ‰æ´»è·ƒç»„å‡ä¸å¯ç”¨ - æ€»å…±å°è¯•äº† %d ä¸ªç«¯ç‚¹ - æœ€åé”™è¯¯: %v", 
			totalEndpointsAttempted, lastErr))
		return nil, fmt.Errorf("all active groups exhausted after trying %d endpoints, last error: %w", totalEndpointsAttempted, lastErr)
	} else {
		// Manual mode: Check if there are other available groups that can be manually activated
		allGroups := rh.endpointManager.GetGroupManager().GetAllGroups()
		availableGroups := make([]string, 0)
		for _, group := range allGroups {
			if !group.IsActive && !rh.endpointManager.GetGroupManager().IsGroupInCooldown(group.Name) {
				// Check if group has healthy endpoints
				healthyInGroup := 0
				for _, ep := range group.Endpoints {
					if ep.IsHealthy() {
						healthyInGroup++
					}
				}
				if healthyInGroup > 0 {
					availableGroups = append(availableGroups, fmt.Sprintf("%s(%dä¸ªå¥åº·ç«¯ç‚¹)", group.Name, healthyInGroup))
				}
			}
		}
		
		if len(availableGroups) > 0 {
			slog.ErrorContext(ctx, fmt.Sprintf("ğŸ’¥ [å½“å‰ç»„ä¸å¯ç”¨] å·²å°è¯• %d ä¸ªç«¯ç‚¹å‡å¤±è´¥ - å¯ç”¨å¤‡ç”¨ç»„: %v - è¯·é€šè¿‡Webç•Œé¢æ‰‹åŠ¨åˆ‡æ¢", 
				totalEndpointsAttempted, availableGroups))
			return nil, fmt.Errorf("current active group exhausted after trying %d endpoints, available backup groups: %v, please switch manually via web interface, last error: %w", 
				totalEndpointsAttempted, availableGroups, lastErr)
		} else {
			slog.ErrorContext(ctx, fmt.Sprintf("ğŸ’¥ [å…¨éƒ¨ä¸å¯ç”¨] æ‰€æœ‰ç»„å‡ä¸å¯ç”¨ - æ€»å…±å°è¯•äº† %d ä¸ªç«¯ç‚¹ - æœ€åé”™è¯¯: %v", 
				totalEndpointsAttempted, lastErr))
			return nil, fmt.Errorf("all groups exhausted or in cooldown after trying %d endpoints, last error: %w", totalEndpointsAttempted, lastErr)
		}
	}
}

// calculateDelay calculates the delay for exponential backoff
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ RetryController.CalculateBackoff æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func (rh *RetryHandler) calculateDelay(attempt int) time.Duration {
	// Calculate exponential backoff: base_delay * (multiplier ^ (attempt - 1))
	multiplier := math.Pow(rh.config.Retry.Multiplier, float64(attempt-1))
	delay := time.Duration(float64(rh.config.Retry.BaseDelay) * multiplier)
	
	// Cap at maximum delay
	if delay > rh.config.Retry.MaxDelay {
		delay = rh.config.Retry.MaxDelay
	}
	
	return delay
}

// shouldRetryStatusCode determines if an HTTP status code should trigger a retry
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ RetryController.ShouldRetry æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func (rh *RetryHandler) shouldRetryStatusCode(statusCode int) *RetryableError {
	switch {
	case statusCode >= 200 && statusCode < 400:
		// 2xx Success and 3xx Redirects - don't retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "è¯·æ±‚æˆåŠŸ",
		}
	case statusCode == 400:
		// 400 Bad Request - should retry (could be temporary issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "è¯·æ±‚æ ¼å¼é”™è¯¯",
		}
	case statusCode == 401:
		// 401 Unauthorized - don't retry (auth issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "èº«ä»½éªŒè¯å¤±è´¥ï¼Œä¸é‡è¯•",
		}
	case statusCode == 403:
		// 403 Forbidden - should retry (permission issue)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "æƒé™ä¸è¶³ï¼Œé‡è¯•ä¸­",
		}
	case statusCode == 404:
		// 404 Not Found - don't retry (resource doesn't exist)
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "èµ„æºä¸å­˜åœ¨ï¼Œä¸é‡è¯•",
		}
	case statusCode == 429:
		// 429 Too Many Requests - should retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "è¯·æ±‚é¢‘ç‡è¿‡é«˜",
		}
	case statusCode >= 400 && statusCode < 500:
		// Other 4xx Client Errors - don't retry by default
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "å®¢æˆ·ç«¯é”™è¯¯ï¼Œä¸é‡è¯•",
		}
	case statusCode >= 500 && statusCode < 600:
		// 5xx Server Errors - should retry
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: true,
			Reason:      "æœåŠ¡å™¨é”™è¯¯",
		}
	default:
		// Unknown status code - don't retry by default
		return &RetryableError{
			StatusCode:  statusCode,
			IsRetryable: false,
			Reason:      "æœªçŸ¥çŠ¶æ€ç ",
		}
	}
}

// IsRetryableError determines if an error should trigger a retry
// @Deprecated: å°†åœ¨v3.3.0ç‰ˆæœ¬ä¸­å®Œå…¨ç§»é™¤
// è¯·ä½¿ç”¨ RetryController.ShouldRetry æ›¿ä»£
// è¿ç§»æŒ‡å—: docs/migration/retry_v3.3.md
func (rh *RetryHandler) IsRetryableError(err error) bool {
	if err == nil {
		return false
	}

	// Handle RetryableError type
	if retryErr, ok := err.(*RetryableError); ok {
		return retryErr.IsRetryable
	}

	// Add logic to determine which errors are retryable
	// For now, we retry all errors except context cancellation
	if err == context.Canceled || err == context.DeadlineExceeded {
		return false
	}

	// Network errors, timeout errors etc. should be retried
	errorStr := strings.ToLower(err.Error())
	if strings.Contains(errorStr, "timeout") ||
		strings.Contains(errorStr, "connection refused") ||
		strings.Contains(errorStr, "connection reset") ||
		strings.Contains(errorStr, "no such host") ||
		strings.Contains(errorStr, "network unreachable") {
		return true
	}

	return true
}

// determineErrorStatus æ ¹æ®é”™è¯¯ç±»å‹å’Œä¸Šä¸‹æ–‡ç¡®å®šçŠ¶æ€
func (rh *RetryHandler) determineErrorStatus(err error, ctx context.Context) string {
	// ä¼˜å…ˆæ£€æŸ¥contextçŠ¶æ€
	if ctx.Err() == context.Canceled {
		return "cancelled"  // ç”¨æˆ·å–æ¶ˆè¯·æ±‚
	}
	if ctx.Err() == context.DeadlineExceeded {
		return "timeout"    // è¯·æ±‚è¶…æ—¶
	}
	
	// æ£€æŸ¥é”™è¯¯æœ¬èº«
	if err != nil {
		if err == context.Canceled {
			return "cancelled"
		}
		if err == context.DeadlineExceeded {
			return "timeout"
		}
		// æ£€æŸ¥é”™è¯¯æ¶ˆæ¯ä¸­çš„å–æ¶ˆæ ‡è¯†
		errorStr := strings.ToLower(err.Error())
		if strings.Contains(errorStr, "context canceled") {
			return "cancelled"
		}
		if strings.Contains(errorStr, "context deadline exceeded") {
			return "timeout"
		}
	}
	
	return "error"  // å…¶ä»–é”™è¯¯
}

// UpdateConfig updates the retry handler configuration
func (rh *RetryHandler) UpdateConfig(cfg *config.Config) {
	rh.config = cfg
}

// GetSuspendedRequestsCount returns the current number of suspended requests
func (rh *RetryHandler) GetSuspendedRequestsCount() int {
	rh.suspendedRequestsMutex.RLock()
	defer rh.suspendedRequestsMutex.RUnlock()
	return rh.suspendedRequestsCount
}

// shouldSuspendRequest determines if a request should be suspended
// æ¡ä»¶ï¼šæ‰‹åŠ¨æ¨¡å¼ + æœ‰å¤‡ç”¨ç»„ + åŠŸèƒ½å¯ç”¨ + æœªè¾¾åˆ°æœ€å¤§æŒ‚èµ·æ•°
func (rh *RetryHandler) shouldSuspendRequest(ctx context.Context) bool {
	// æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
	if !rh.config.RequestSuspend.Enabled {
		slog.InfoContext(ctx, "ğŸ” [æŒ‚èµ·æ£€æŸ¥] åŠŸèƒ½æœªå¯ç”¨ï¼Œä¸æŒ‚èµ·è¯·æ±‚")
		return false
	}
	
	// æ£€æŸ¥æ˜¯å¦ä¸ºæ‰‹åŠ¨æ¨¡å¼
	if rh.config.Group.AutoSwitchBetweenGroups {
		slog.InfoContext(ctx, "ğŸ” [æŒ‚èµ·æ£€æŸ¥] å½“å‰ä¸ºè‡ªåŠ¨åˆ‡æ¢æ¨¡å¼ï¼Œä¸æŒ‚èµ·è¯·æ±‚")
		return false
	}
	
	// æ£€æŸ¥å½“å‰æŒ‚èµ·è¯·æ±‚æ•°é‡æ˜¯å¦è¶…è¿‡é™åˆ¶
	rh.suspendedRequestsMutex.RLock()
	currentCount := rh.suspendedRequestsCount
	rh.suspendedRequestsMutex.RUnlock()
	
	if currentCount >= rh.config.RequestSuspend.MaxSuspendedRequests {
		slog.WarnContext(ctx, fmt.Sprintf("ğŸš« [æŒ‚èµ·é™åˆ¶] å½“å‰æŒ‚èµ·è¯·æ±‚æ•° %d å·²è¾¾åˆ°æœ€å¤§é™åˆ¶ %dï¼Œä¸å†æŒ‚èµ·æ–°è¯·æ±‚", 
			currentCount, rh.config.RequestSuspend.MaxSuspendedRequests))
		return false
	}
	
	// æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯ç”¨çš„å¤‡ç”¨ç»„
	allGroups := rh.endpointManager.GetGroupManager().GetAllGroups()
	hasAvailableBackupGroups := false
	availableGroups := []string{}
	
	slog.InfoContext(ctx, fmt.Sprintf("ğŸ” [æŒ‚èµ·æ£€æŸ¥] å¼€å§‹æ£€æŸ¥å¯ç”¨å¤‡ç”¨ç»„ï¼Œæ€»å…± %d ä¸ªç»„", len(allGroups)))
	
	for _, group := range allGroups {
		slog.InfoContext(ctx, fmt.Sprintf("ğŸ” [æŒ‚èµ·æ£€æŸ¥] æ£€æŸ¥ç»„ %s: IsActive=%t, InCooldown=%t", 
			group.Name, group.IsActive, rh.endpointManager.GetGroupManager().IsGroupInCooldown(group.Name)))
		
		// æ£€æŸ¥éæ´»è·ƒç»„ä¸”ä¸åœ¨å†·å´æœŸçš„ç»„
		if !group.IsActive && !rh.endpointManager.GetGroupManager().IsGroupInCooldown(group.Name) {
			// æ£€æŸ¥ç»„å†…æ˜¯å¦æœ‰å¥åº·ç«¯ç‚¹
			healthyCount := 0
			for _, ep := range group.Endpoints {
				if ep.IsHealthy() {
					healthyCount++
				}
			}
			slog.InfoContext(ctx, fmt.Sprintf("ğŸ” [æŒ‚èµ·æ£€æŸ¥] ç»„ %s å¥åº·ç«¯ç‚¹æ•°: %d", group.Name, healthyCount))
			
			if healthyCount > 0 {
				hasAvailableBackupGroups = true
				availableGroups = append(availableGroups, fmt.Sprintf("%s(%dä¸ªå¥åº·ç«¯ç‚¹)", group.Name, healthyCount))
			}
		}
	}
	
	if !hasAvailableBackupGroups {
		slog.InfoContext(ctx, "ğŸ” [æŒ‚èµ·æ£€æŸ¥] æ²¡æœ‰å¯ç”¨çš„å¤‡ç”¨ç»„ï¼Œä¸æŒ‚èµ·è¯·æ±‚")
		return false
	}
	
	slog.InfoContext(ctx, fmt.Sprintf("âœ… [æŒ‚èµ·æ£€æŸ¥] æ»¡è¶³æŒ‚èµ·æ¡ä»¶: æ‰‹åŠ¨æ¨¡å¼=%t, åŠŸèƒ½å¯ç”¨=%t, å½“å‰æŒ‚èµ·æ•°=%d/%d, å¯ç”¨å¤‡ç”¨ç»„=%v", 
		!rh.config.Group.AutoSwitchBetweenGroups, rh.config.RequestSuspend.Enabled, 
		currentCount, rh.config.RequestSuspend.MaxSuspendedRequests, availableGroups))
	
	return true
}

// waitForGroupSwitch suspends the request and waits for group switch notification
// æŒ‚èµ·è¯·æ±‚ç­‰å¾…ç»„åˆ‡æ¢ï¼Œè¿”å›æ˜¯å¦æˆåŠŸåˆ‡æ¢åˆ°æ–°ç»„
func (rh *RetryHandler) waitForGroupSwitch(ctx context.Context, connID string) bool {
	// å¢åŠ æŒ‚èµ·è¯·æ±‚è®¡æ•°
	rh.suspendedRequestsMutex.Lock()
	rh.suspendedRequestsCount++
	currentCount := rh.suspendedRequestsCount
	rh.suspendedRequestsMutex.Unlock()
	
	// ç¡®ä¿åœ¨é€€å‡ºæ—¶å‡å°‘è®¡æ•°
	defer func() {
		rh.suspendedRequestsMutex.Lock()
		rh.suspendedRequestsCount--
		newCount := rh.suspendedRequestsCount
		rh.suspendedRequestsMutex.Unlock()
		slog.InfoContext(ctx, fmt.Sprintf("â¬‡ï¸ [æŒ‚èµ·ç»“æŸ] è¿æ¥ %s è¯·æ±‚æŒ‚èµ·ç»“æŸï¼Œå½“å‰æŒ‚èµ·æ•°: %d", connID, newCount))
	}()
	
	slog.InfoContext(ctx, fmt.Sprintf("â¸ï¸ [è¯·æ±‚æŒ‚èµ·] è¿æ¥ %s è¯·æ±‚å·²æŒ‚èµ·ï¼Œç­‰å¾…ç»„åˆ‡æ¢ (å½“å‰æŒ‚èµ·æ•°: %d)", connID, currentCount))
	
	// çŠ¶æ€ç®¡ç†å·²è¿ç§»åˆ°LifecycleManagerï¼Œæ­¤å¤„ä¸å†è®°å½•çŠ¶æ€
	// å†å²æ³¨é‡Šï¼šæ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºæŒ‚èµ·çŠ¶æ€
	
	// è®¢é˜…ç»„åˆ‡æ¢é€šçŸ¥
	groupChangeNotify := rh.endpointManager.GetGroupManager().SubscribeToGroupChanges()
	defer func() {
		// ç¡®ä¿æ¸…ç†è®¢é˜…ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
		rh.endpointManager.GetGroupManager().UnsubscribeFromGroupChanges(groupChangeNotify)
		slog.DebugContext(ctx, fmt.Sprintf("ğŸ”Œ [è®¢é˜…æ¸…ç†] è¿æ¥ %s ç»„åˆ‡æ¢é€šçŸ¥è®¢é˜…å·²æ¸…ç†", connID))
	}()
	
	// åˆ›å»ºè¶…æ—¶context
	timeout := rh.config.RequestSuspend.Timeout
	if timeout <= 0 {
		timeout = 300 * time.Second // é»˜è®¤5åˆ†é’Ÿ
	}
	
	timeoutCtx, timeoutCancel := context.WithTimeout(ctx, timeout)
	defer timeoutCancel()
	
	slog.InfoContext(ctx, fmt.Sprintf("â° [æŒ‚èµ·è¶…æ—¶] è¿æ¥ %s æŒ‚èµ·è¶…æ—¶è®¾ç½®: %vï¼Œç­‰å¾…ç»„åˆ‡æ¢é€šçŸ¥...", connID, timeout))
	
	// ç­‰å¾…ç»„åˆ‡æ¢é€šçŸ¥æˆ–è¶…æ—¶
	select {
	case newGroupName := <-groupChangeNotify:
		// æ”¶åˆ°ç»„åˆ‡æ¢é€šçŸ¥
		slog.InfoContext(ctx, fmt.Sprintf("ğŸ“¡ [ç»„åˆ‡æ¢é€šçŸ¥] è¿æ¥ %s æ”¶åˆ°ç»„åˆ‡æ¢é€šçŸ¥: %sï¼ŒéªŒè¯æ–°ç»„å¯ç”¨æ€§", connID, newGroupName))
		
		// éªŒè¯æ–°æ¿€æ´»çš„ç»„æ˜¯å¦æœ‰å¥åº·ç«¯ç‚¹
		newEndpoints := rh.endpointManager.GetHealthyEndpoints()
		if len(newEndpoints) > 0 {
			slog.InfoContext(ctx, fmt.Sprintf("âœ… [åˆ‡æ¢æˆåŠŸ] è¿æ¥ %s æ–°ç»„ %s æœ‰ %d ä¸ªå¥åº·ç«¯ç‚¹ï¼Œæ¢å¤è¯·æ±‚å¤„ç†", 
				connID, newGroupName, len(newEndpoints)))
			return true
		} else {
			slog.WarnContext(ctx, fmt.Sprintf("âš ï¸ [åˆ‡æ¢æ— æ•ˆ] è¿æ¥ %s æ–°ç»„ %s æš‚æ— å¥åº·ç«¯ç‚¹ï¼ŒæŒ‚èµ·å¤±è´¥", 
				connID, newGroupName))
			return false
		}
		
	case <-timeoutCtx.Done():
		// æŒ‚èµ·è¶…æ—¶
		if timeoutCtx.Err() == context.DeadlineExceeded {
			slog.WarnContext(ctx, fmt.Sprintf("â° [æŒ‚èµ·è¶…æ—¶] è¿æ¥ %s æŒ‚èµ·ç­‰å¾…è¶…æ—¶ (%v)ï¼Œåœæ­¢ç­‰å¾…", connID, timeout))
		} else {
			slog.InfoContext(ctx, fmt.Sprintf("ğŸ”„ [ä¸Šä¸‹æ–‡å–æ¶ˆ] è¿æ¥ %s æŒ‚èµ·æœŸé—´ä¸Šä¸‹æ–‡è¢«å–æ¶ˆ", connID))
		}
		return false
		
	case <-ctx.Done():
		// åŸå§‹è¯·æ±‚è¢«å–æ¶ˆ
		ctxErr := ctx.Err()
		if ctxErr == context.Canceled {
			slog.InfoContext(ctx, fmt.Sprintf("âŒ [è¯·æ±‚å–æ¶ˆ] è¿æ¥ %s åŸå§‹è¯·æ±‚è¢«å®¢æˆ·ç«¯å–æ¶ˆï¼Œç»“æŸæŒ‚èµ·", connID))
		} else if ctxErr == context.DeadlineExceeded {
			slog.InfoContext(ctx, fmt.Sprintf("â° [è¯·æ±‚è¶…æ—¶] è¿æ¥ %s åŸå§‹è¯·æ±‚ä¸Šä¸‹æ–‡è¶…æ—¶ï¼Œç»“æŸæŒ‚èµ·", connID))
		} else {
			slog.InfoContext(ctx, fmt.Sprintf("âŒ [è¯·æ±‚å¼‚å¸¸] è¿æ¥ %s åŸå§‹è¯·æ±‚ä¸Šä¸‹æ–‡å¼‚å¸¸: %vï¼Œç»“æŸæŒ‚èµ·", connID, ctxErr))
		}
		return false
	}
}