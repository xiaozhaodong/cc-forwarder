package proxy

import (
	"bufio"
	"context"
	"fmt"
	"io"
	"log/slog"
	"net/http"
	"strings"
	"sync"
	"time"

	"cc-forwarder/internal/tracking"
)

// ç¼“å†²åŒºå¤§å°å¸¸é‡
const (
	StreamBufferSize     = 8192  // 8KBä¸»ç¼“å†²åŒº
	LineBufferInitSize   = 1024  // 1KBè¡Œç¼“å†²åŒºåˆå§‹å¤§å°
	BackgroundBufferSize = 4096  // 4KBåå°è§£æç¼“å†²åŒº
)

// StreamProcessor æµå¼å¤„ç†å™¨æ ¸å¿ƒç»“æ„ä½“
type StreamProcessor struct {
	// æ ¸å¿ƒç»„ä»¶
	tokenParser    *TokenParser              // Tokenè§£æå™¨ï¼Œç”¨äºæå–æ¨¡å‹ä¿¡æ¯å’Œä½¿ç”¨ç»Ÿè®¡
	usageTracker   *tracking.UsageTracker    // ä½¿ç”¨è·Ÿè¸ªå™¨ï¼Œè®°å½•è¯·æ±‚ç”Ÿå‘½å‘¨æœŸ
	responseWriter http.ResponseWriter       // HTTPå“åº”å†™å…¥å™¨
	flusher        http.Flusher              // HTTPåˆ·æ–°å™¨ï¼Œç”¨äºç«‹å³å‘é€æ•°æ®åˆ°å®¢æˆ·ç«¯
	
	// é”™è¯¯å¤„ç†å’Œæ¢å¤
	errorRecovery  *ErrorRecoveryManager     // é”™è¯¯æ¢å¤ç®¡ç†å™¨
	
	// è¯·æ±‚æ ‡è¯†ä¿¡æ¯
	requestID      string                    // è¯·æ±‚å”¯ä¸€æ ‡è¯†ç¬¦
	endpoint       string                    // ç«¯ç‚¹åç§°
	
	// æµå¼å¤„ç†çŠ¶æ€
	startTime      time.Time                 // å¤„ç†å¼€å§‹æ—¶é—´
	bytesProcessed int64                     // å·²å¤„ç†å­—èŠ‚æ•°
	lineBuffer     []byte                    // SSEè¡Œç¼“å†²åŒº
	partialData    []byte                    // éƒ¨åˆ†æ•°æ®ç¼“å†²åŒºï¼Œç”¨äºé”™è¯¯æ¢å¤
	
	// å¹¶å‘æ§åˆ¶
	parseWg        sync.WaitGroup            // ç­‰å¾…ç»„ï¼Œç¡®ä¿åå°è§£æå®Œæˆ
	parseMutex     sync.Mutex               // è§£æäº’æ–¥é”ï¼Œä¿æŠ¤å…±äº«çŠ¶æ€
	
	// é”™è¯¯å¤„ç†
	parseErrors    []error                   // è§£æè¿‡ç¨‹ä¸­çš„é”™è¯¯é›†åˆ
	maxParseErrors int                       // æœ€å¤§å…è®¸è§£æé”™è¯¯æ•°
	
	// å®ŒæˆçŠ¶æ€è·Ÿè¸ª
	completionRecorded bool                  // æ˜¯å¦å·²ç»è®°å½•å®ŒæˆçŠ¶æ€ï¼Œé˜²æ­¢é‡å¤è®°å½•
}

// NewStreamProcessor åˆ›å»ºæ–°çš„æµå¼å¤„ç†å™¨å®ä¾‹
func NewStreamProcessor(tokenParser *TokenParser, usageTracker *tracking.UsageTracker, 
	w http.ResponseWriter, flusher http.Flusher, requestID, endpoint string) *StreamProcessor {
	
	return &StreamProcessor{
		tokenParser:    tokenParser,
		usageTracker:   usageTracker,
		responseWriter: w,
		flusher:        flusher,
		errorRecovery:  NewErrorRecoveryManager(usageTracker),
		requestID:      requestID,
		endpoint:       endpoint,
		startTime:      time.Now(),
		lineBuffer:     make([]byte, 0, LineBufferInitSize),
		partialData:    make([]byte, 0, BackgroundBufferSize),
		maxParseErrors: 10, // æœ€å¤šå…è®¸10ä¸ªè§£æé”™è¯¯
	}
}

// ProcessStream å®ç°è¾¹æ¥æ”¶è¾¹è½¬å‘çš„8KBç¼“å†²åŒºæµå¼å¤„ç†
// è¿™æ˜¯æ ¸å¿ƒæ–¹æ³•ï¼Œå®ç°çœŸæ­£çš„æµå¼å¤„ç†æœºåˆ¶
func (sp *StreamProcessor) ProcessStream(ctx context.Context, resp *http.Response) error {
	defer resp.Body.Close()
	defer sp.waitForBackgroundParsing() // ç¡®ä¿æ‰€æœ‰åå°è§£æå®Œæˆ
	
	// åˆå§‹åŒ–8KBç¼“å†²åŒº
	buffer := make([]byte, StreamBufferSize)
	reader := bufio.NewReader(resp.Body)
	
	// è®°å½•æµå¤„ç†å¼€å§‹
	slog.Info(fmt.Sprintf("ğŸŒŠ [æµå¼å¤„ç†] [%s] å¼€å§‹æµå¼å¤„ç†ï¼Œç«¯ç‚¹: %s", sp.requestID, sp.endpoint))
	
	// ä¸»æµå¼å¤„ç†å¾ªç¯
	for {
		// æ£€æŸ¥contextå–æ¶ˆ - ä¼˜å…ˆçº§æœ€é«˜
		select {
		case <-ctx.Done():
			// å®¢æˆ·ç«¯å–æ¶ˆï¼Œè¿›å…¥ä¼˜é›…å–æ¶ˆå¤„ç†
			return sp.handleCancellation(ctx, ctx.Err())
		default:
			// ç»§ç»­æ­£å¸¸å¤„ç†
		}
		
		// 1. ä»å“åº”ä¸­è¯»å–æ•°æ®åˆ°8KBç¼“å†²åŒº
		n, err := reader.Read(buffer)
		
		if n > 0 {
			chunk := buffer[:n]
			
			// ä¿å­˜éƒ¨åˆ†æ•°æ®ç”¨äºé”™è¯¯æ¢å¤
			sp.savePartialData(chunk)
			
			// 2. ç«‹å³è½¬å‘åˆ°å®¢æˆ·ç«¯ - è¿™æ˜¯å…³é”®ï¼ä¸ç­‰å¾…å®Œæ•´å“åº”
			if writeErr := sp.forwardToClient(chunk); writeErr != nil {
				// ä½¿ç”¨é”™è¯¯æ¢å¤ç®¡ç†å™¨å¤„ç†è½¬å‘é”™è¯¯
				errorCtx := sp.errorRecovery.ClassifyError(writeErr, sp.requestID, sp.endpoint, "", 0)
				sp.errorRecovery.HandleFinalFailure(errorCtx)
				slog.Error(fmt.Sprintf("âŒ [æµå¼é”™è¯¯] [%s] è½¬å‘åˆ°å®¢æˆ·ç«¯å¤±è´¥: %v", sp.requestID, writeErr))
				return fmt.Errorf("failed to forward to client: %w", writeErr)
			}
			
			// 3. å¹¶è¡Œè§£æTokenä¿¡æ¯ - ä¸å½±å“è½¬å‘æ€§èƒ½
			sp.parseTokensInBackground(chunk)
			
			// 4. æ›´æ–°å¤„ç†çŠ¶æ€
			sp.bytesProcessed += int64(n)
		}
		
		// å¤„ç†è¯»å–ç»“æŸå’Œé”™è¯¯
		if err == io.EOF {
			// ç­‰å¾…æ‰€æœ‰åå°è§£æå®Œæˆ
			sp.waitForBackgroundParsing()
			
			// æ£€æŸ¥æ˜¯å¦å·²ç»é€šè¿‡SSEè§£æè®°å½•äº†å®ŒæˆçŠ¶æ€ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨fallback
			sp.ensureRequestCompletion()
			
			slog.Info(fmt.Sprintf("âœ… [æµå¼å®Œæˆ] [%s] ç«¯ç‚¹: %s, æµå¤„ç†æ­£å¸¸å®Œæˆï¼Œå·²å¤„ç† %d å­—èŠ‚", 
				sp.requestID, sp.endpoint, sp.bytesProcessed))
			return nil
		}
		
		if err != nil {
			// ç½‘ç»œä¸­æ–­æˆ–å…¶ä»–é”™è¯¯ï¼Œå°è¯•éƒ¨åˆ†æ•°æ®å¤„ç†
			return sp.handlePartialStream(err)
		}
	}
}

// forwardToClient ç«‹å³è½¬å‘æ•°æ®åˆ°å®¢æˆ·ç«¯
func (sp *StreamProcessor) forwardToClient(data []byte) error {
	// å†™å…¥æ•°æ®åˆ°å“åº”
	if _, err := sp.responseWriter.Write(data); err != nil {
		return err
	}
	
	// ç«‹å³åˆ·æ–°ï¼Œç¡®ä¿æ•°æ®ç«‹å³å‘é€åˆ°å®¢æˆ·ç«¯
	sp.flusher.Flush()
	
	return nil
}

// parseTokensInBackground å¹¶å‘Tokenè§£æï¼Œä¸é˜»å¡ä¸»æµ
// è¿™ä¸ªæ–¹æ³•åœ¨åå°goroutineä¸­è§£æSSEäº‹ä»¶ï¼Œæå–æ¨¡å‹ä¿¡æ¯å’ŒTokenä½¿ç”¨ç»Ÿè®¡
func (sp *StreamProcessor) parseTokensInBackground(data []byte) {
	// ä¸ºæ¯ä¸ªæ•°æ®å—å¯åŠ¨ä¸€ä¸ªåå°goroutine
	sp.parseWg.Add(1)
	
	go func() {
		defer sp.parseWg.Done()
		
		// åˆ›å»ºåå°å¤„ç†ç¼“å†²åŒº
		parseBuffer := make([]byte, len(data))
		copy(parseBuffer, data)
		
		// é€å­—èŠ‚å¤„ç†ï¼Œæ„å»ºSSEè¡Œ
		sp.parseMutex.Lock()
		defer sp.parseMutex.Unlock()
		
		for _, b := range parseBuffer {
			// æ„å»ºè¡Œç¼“å†²åŒº
			sp.lineBuffer = append(sp.lineBuffer, b)
			
			// æ£€æµ‹æ¢è¡Œç¬¦ï¼Œå¤„ç†å®Œæ•´çš„SSEè¡Œ
			if b == '\n' {
				line := strings.TrimSpace(string(sp.lineBuffer))
				
				// âœ… ä¿®å¤ï¼šå¤„ç†æ‰€æœ‰è¡Œï¼ŒåŒ…æ‹¬ç©ºè¡Œï¼ˆç©ºè¡Œè§¦å‘SSEäº‹ä»¶è§£æï¼‰
				sp.processSSELine(line)
				
				// é‡ç½®è¡Œç¼“å†²åŒºï¼Œå‡†å¤‡ä¸‹ä¸€è¡Œ
				sp.lineBuffer = sp.lineBuffer[:0]
			}
		}
	}()
}

// processSSELine å¤„ç†å•ä¸ªSSEè¡Œ
func (sp *StreamProcessor) processSSELine(line string) {
	// ä½¿ç”¨ç°æœ‰çš„TokenParserè¿›è¡Œè§£æ
	tokenUsage := sp.tokenParser.ParseSSELine(line)
	
	if tokenUsage != nil {
		// è§£ææˆåŠŸï¼Œè½¬æ¢ä¸ºtracking.TokenUsageå¹¶è®°å½•åˆ°usage tracker
		trackingTokens := &tracking.TokenUsage{
			InputTokens:          int64(tokenUsage.InputTokens),
			OutputTokens:         int64(tokenUsage.OutputTokens),
			CacheCreationTokens:  int64(tokenUsage.CacheCreationTokens),
			CacheReadTokens:      int64(tokenUsage.CacheReadTokens),
		}
		
		// è·å–æ¨¡å‹åç§°
		modelName := sp.tokenParser.GetModelName()
		if modelName == "" {
			modelName = "default"
		}
		
		// è®°å½•å®ŒæˆçŠ¶æ€åˆ°usage tracker
		if sp.usageTracker != nil && sp.requestID != "" && !sp.completionRecorded {
			duration := time.Since(sp.startTime)
			sp.usageTracker.RecordRequestComplete(sp.requestID, modelName, trackingTokens, duration)
			sp.usageTracker.RecordRequestUpdate(sp.requestID, sp.endpoint, "", "completed", 0, 0)
			sp.completionRecorded = true  // æ ‡è®°å·²è®°å½•å®ŒæˆçŠ¶æ€
			
			slog.Info(fmt.Sprintf("ğŸª™ [Tokenä½¿ç”¨ç»Ÿè®¡] [%s] ä»æµå¼è§£æä¸­æå–å®Œæ•´ä»¤ç‰Œä½¿ç”¨æƒ…å†µ - æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d, ç¼“å­˜åˆ›å»º: %d, ç¼“å­˜è¯»å–: %d", 
				sp.requestID, modelName, trackingTokens.InputTokens, trackingTokens.OutputTokens, trackingTokens.CacheCreationTokens, trackingTokens.CacheReadTokens))
		}
	}
}

// ensureRequestCompletion ç¡®ä¿è¯·æ±‚å®ŒæˆçŠ¶æ€è¢«è®°å½•ï¼ˆfallbackæœºåˆ¶ï¼‰
func (sp *StreamProcessor) ensureRequestCompletion() {
	sp.parseMutex.Lock()
	defer sp.parseMutex.Unlock()
	
	if sp.usageTracker != nil && sp.requestID != "" && !sp.completionRecorded {
		// å¦‚æœè¿˜æ²¡æœ‰è®°å½•å®ŒæˆçŠ¶æ€ï¼Œä½¿ç”¨fallbackæ–¹å¼è®°å½•
		duration := time.Since(sp.startTime)
		
		// å°è¯•ä»TokenParserè·å–æœ€ç»ˆä½¿ç”¨ç»Ÿè®¡
		finalUsage := sp.tokenParser.GetFinalUsage()
		modelName := sp.tokenParser.GetModelName()
		
		if finalUsage != nil && modelName != "" {
			// æœ‰å®Œæ•´çš„tokenå’Œæ¨¡å‹ä¿¡æ¯
			sp.usageTracker.RecordRequestComplete(sp.requestID, modelName, finalUsage, duration)
			slog.Info(fmt.Sprintf("ğŸª™ [Tokenä½¿ç”¨ç»Ÿè®¡] [%s] ä»TokenParserè·å–æœ€ç»ˆä»¤ç‰Œä½¿ç”¨æƒ…å†µ - æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d", 
				sp.requestID, modelName, finalUsage.InputTokens, finalUsage.OutputTokens))
		} else {
			// æ²¡æœ‰tokenä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼è®°å½•å®ŒæˆçŠ¶æ€
			emptyTokens := &tracking.TokenUsage{
				InputTokens: 0, OutputTokens: 0, 
				CacheCreationTokens: 0, CacheReadTokens: 0,
			}
			defaultModel := "default"
			if modelName != "" {
				defaultModel = modelName
			}
			
			sp.usageTracker.RecordRequestComplete(sp.requestID, defaultModel, emptyTokens, duration)
			slog.Info(fmt.Sprintf("ğŸ¯ [æ— Tokenå®Œæˆ] [%s] æµå¼å“åº”ä¸åŒ…å«tokenä¿¡æ¯ï¼Œæ ‡è®°ä¸ºå®Œæˆï¼Œæ¨¡å‹: %s", 
				sp.requestID, defaultModel))
		}
		
		sp.usageTracker.RecordRequestUpdate(sp.requestID, sp.endpoint, "", "completed", 0, 0)
		sp.completionRecorded = true
	}
}

// handlePartialStream å¤„ç†éƒ¨åˆ†æ•°æ®æµä¸­æ–­æƒ…å†µï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
// å½“ç½‘ç»œä¸­æ–­æˆ–å…¶ä»–é”™è¯¯å‘ç”Ÿæ—¶ï¼Œä¸å†è¿›è¡Œé”™è¯¯åˆ†ç±»ï¼Œè®©ä¸Šå±‚ç»Ÿä¸€å¤„ç†
func (sp *StreamProcessor) handlePartialStream(err error) error {
	// ä»…è®°å½•æµå¤„ç†ä¸­æ–­ï¼Œä¸å†è¿›è¡Œé”™è¯¯åˆ†ç±»
	slog.Warn(fmt.Sprintf("âš ï¸ [æµå¼ä¸­æ–­] [%s] æµå¤„ç†ä¸­æ–­: %v, å·²å¤„ç† %d å­—èŠ‚. é”™è¯¯å°†ç”±ä¸Šå±‚ç»Ÿä¸€å¤„ç†.", 
		sp.requestID, err, sp.bytesProcessed))
	
	// ç­‰å¾…æ‰€æœ‰åå°è§£æå®Œæˆ
	sp.waitForBackgroundParsing()
	
	// å°è¯•ä»éƒ¨åˆ†æ•°æ®ä¸­æ¢å¤æœ‰ç”¨ä¿¡æ¯
	if len(sp.partialData) > 0 {
		sp.errorRecovery.RecoverFromPartialData(sp.requestID, sp.partialData, time.Since(sp.startTime))
	}
	
	// æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„Tokenæ•°æ®ï¼Œå¹¶è®°å½•éƒ¨åˆ†å®Œæˆ
	if sp.usageTracker != nil && sp.requestID != "" {
		duration := time.Since(sp.startTime)
		emptyTokens := &tracking.TokenUsage{
			InputTokens: 0, OutputTokens: 0, 
			CacheCreationTokens: 0, CacheReadTokens: 0,
		}
		modelName := "partial_stream"
		if sp.tokenParser.modelName != "" {
			modelName = sp.tokenParser.modelName + "_partial"
		}
		sp.usageTracker.RecordRequestComplete(sp.requestID, modelName, emptyTokens, duration)
		sp.usageTracker.RecordRequestUpdate(sp.requestID, sp.endpoint, "", "partial_complete", 0, 0)
		
		slog.Info(fmt.Sprintf("ğŸ’¾ [éƒ¨åˆ†ä¿å­˜] [%s] éƒ¨åˆ†æµå¼æ•°æ®å·²ä¿å­˜ï¼Œæ¨¡å‹: %s", 
			sp.requestID, modelName))
	}
	
	// ç›´æ¥è¿”å›é”™è¯¯ï¼Œè®©è°ƒç”¨è€…(handler)æ¥åˆ†ç±»å’Œå¤„ç†æœ€ç»ˆå¤±è´¥
	return err
}

// ProcessStreamWithRetry æ”¯æŒç½‘ç»œä¸­æ–­æ¢å¤çš„æµå¼å¤„ç†ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
// åœ¨ç½‘ç»œä¸ç¨³å®šç¯å¢ƒä¸‹æä¾›æ™ºèƒ½é‡è¯•æœºåˆ¶
func (sp *StreamProcessor) ProcessStreamWithRetry(ctx context.Context, resp *http.Response) error {
	const maxRetries = 3
	
	for attempt := 0; attempt <= maxRetries; attempt++ {
		// åˆ†ç±»å½“å‰å°è¯•çš„é”™è¯¯ä¸Šä¸‹æ–‡
		var lastErr error
		
		if attempt > 0 {
			// ä½¿ç”¨é”™è¯¯æ¢å¤ç®¡ç†å™¨è®¡ç®—é‡è¯•å»¶è¿Ÿ
			errorCtx := sp.errorRecovery.ClassifyError(lastErr, sp.requestID, sp.endpoint, "", attempt)
			
			// æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
			if !sp.errorRecovery.ShouldRetry(errorCtx) {
				slog.Info(fmt.Sprintf("ğŸ›‘ [é‡è¯•åœæ­¢] [%s] é”™è¯¯æ¢å¤ç®¡ç†å™¨å»ºè®®åœæ­¢é‡è¯•", sp.requestID))
				sp.errorRecovery.HandleFinalFailure(errorCtx)
				return lastErr
			}
			
			// æ‰§è¡Œé‡è¯•å»¶è¿Ÿ
			if retryErr := sp.errorRecovery.ExecuteRetry(ctx, errorCtx); retryErr != nil {
				return retryErr
			}
		}
		
		// å°è¯•æµå¼å¤„ç†
		err := sp.ProcessStream(ctx, resp)
		
		if err == nil {
			// å¤„ç†æˆåŠŸ
			if attempt > 0 {
				slog.Info(fmt.Sprintf("âœ… [é‡è¯•æˆåŠŸ] [%s] ç¬¬ %d æ¬¡é‡è¯•æˆåŠŸ", sp.requestID, attempt))
			}
			return nil
		}
		
		lastErr = err
		
		// ç®€åŒ–çš„é‡è¯•åˆ¤æ–­é€»è¾‘ï¼Œé¿å…é‡å¤é”™è¯¯åˆ†ç±»
		// å¯¹äºæµå¼å¤„ç†ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ç½‘ç»œç›¸å…³çš„é”™è¯¯æ˜¯å¦å¯é‡è¯•
		shouldRetry := false
		if err != nil {
			errStr := strings.ToLower(err.Error())
			// ç®€å•åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„ç½‘ç»œ/è¶…æ—¶é”™è¯¯
			if strings.Contains(errStr, "timeout") || 
			   strings.Contains(errStr, "connection") || 
			   strings.Contains(errStr, "network") ||
			   strings.Contains(errStr, "reset") ||
			   strings.Contains(errStr, "refused") {
				shouldRetry = true
			}
		}
		
		if shouldRetry && attempt < maxRetries {
			slog.Warn(fmt.Sprintf("ğŸ”„ [ç½‘ç»œé”™è¯¯é‡è¯•] [%s] ç½‘ç»œç›¸å…³é”™è¯¯å°†é‡è¯•: %v", sp.requestID, err))
			continue
		}
		
		// ä¸å¯é‡è¯•é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°å·²æ»¡ï¼Œç›´æ¥è¿”å›è®©ä¸Šå±‚å¤„ç†
		slog.Info(fmt.Sprintf("ğŸ›‘ [é‡è¯•åœæ­¢] [%s] %d æ¬¡é‡è¯•ååœæ­¢ï¼Œé”™è¯¯å°†ç”±ä¸Šå±‚å¤„ç†: %v", 
			sp.requestID, attempt, err))
		return err
	}
	
	// åˆ›å»ºæœ€ç»ˆå¤±è´¥çš„é”™è¯¯ä¸Šä¸‹æ–‡
	finalErrorCtx := &ErrorContext{
		RequestID:     sp.requestID,
		EndpointName:  sp.endpoint,
		AttemptCount:  maxRetries,
		ErrorType:     ErrorTypeUnknown,
		OriginalError: fmt.Errorf("stream processing failed after %d retries", maxRetries),
	}
	sp.errorRecovery.HandleFinalFailure(finalErrorCtx)
	
	return fmt.Errorf("stream processing failed after %d retries", maxRetries)
}

// waitForBackgroundParsing ç­‰å¾…æ‰€æœ‰åå°è§£æå®Œæˆ
func (sp *StreamProcessor) waitForBackgroundParsing() {
	// ç­‰å¾…æ‰€æœ‰åå°goroutineå®Œæˆ
	sp.parseWg.Wait()
	
	// å¤„ç†å‰©ä½™çš„è¡Œç¼“å†²åŒºæ•°æ®
	sp.parseMutex.Lock()
	if len(sp.lineBuffer) > 0 {
		line := strings.TrimSpace(string(sp.lineBuffer))
		if len(line) > 0 {
			sp.processSSELine(line)
		}
		sp.lineBuffer = sp.lineBuffer[:0]
	}
	sp.parseMutex.Unlock()
}

// savePartialData ä¿å­˜éƒ¨åˆ†æ•°æ®ç”¨äºé”™è¯¯æ¢å¤
func (sp *StreamProcessor) savePartialData(chunk []byte) {
	// é™åˆ¶éƒ¨åˆ†æ•°æ®ç¼“å†²åŒºå¤§å°ï¼Œé˜²æ­¢å†…å­˜è¿‡åº¦ä½¿ç”¨
	const maxPartialDataSize = 64 * 1024 // 64KB

	sp.parseMutex.Lock()
	defer sp.parseMutex.Unlock()

	// å¦‚æœæ·»åŠ æ–°æ•°æ®ä¼šè¶…è¿‡é™åˆ¶ï¼Œåˆ™ç§»é™¤æ—§æ•°æ®
	if len(sp.partialData)+len(chunk) > maxPartialDataSize {
		// ä¿ç•™æœ€åçš„32KBæ•°æ®ï¼Œä¸¢å¼ƒæ›´æ—©çš„æ•°æ®
		keepSize := maxPartialDataSize/2 - len(chunk)
		if keepSize > 0 && keepSize < len(sp.partialData) {
			copy(sp.partialData, sp.partialData[len(sp.partialData)-keepSize:])
			sp.partialData = sp.partialData[:keepSize]
		} else {
			sp.partialData = sp.partialData[:0]
		}
	}

	// æ·»åŠ æ–°çš„æ•°æ®å—
	sp.partialData = append(sp.partialData, chunk...)
}

// GetProcessingStats è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯
func (sp *StreamProcessor) GetProcessingStats() map[string]interface{} {
	return map[string]interface{}{
		"request_id":       sp.requestID,
		"endpoint":         sp.endpoint, 
		"bytes_processed":  sp.bytesProcessed,
		"processing_time":  time.Since(sp.startTime),
		"parse_errors":     len(sp.parseErrors),
		"max_parse_errors": sp.maxParseErrors,
	}
}

// Reset é‡ç½®å¤„ç†å™¨çŠ¶æ€ï¼Œç”¨äºå¤ç”¨
func (sp *StreamProcessor) Reset() {
	sp.parseMutex.Lock()
	defer sp.parseMutex.Unlock()
	
	sp.startTime = time.Now()
	sp.bytesProcessed = 0
	sp.lineBuffer = sp.lineBuffer[:0]
	sp.partialData = sp.partialData[:0] // é‡ç½®éƒ¨åˆ†æ•°æ®ç¼“å†²åŒº
	sp.parseErrors = sp.parseErrors[:0]
	
	// é‡ç½®TokenParserçŠ¶æ€
	if sp.tokenParser != nil {
		sp.tokenParser.Reset()
	}
	
	slog.Info(fmt.Sprintf("ğŸ”„ [å¤„ç†å™¨é‡ç½®] [%s] æµå¤„ç†å™¨å·²é‡ç½®", sp.requestID))
}

// handleCancellation å¤„ç†å®¢æˆ·ç«¯å–æ¶ˆè¯·æ±‚ - Phase 2 ä¼˜é›…å–æ¶ˆå¤„ç†å™¨
func (sp *StreamProcessor) handleCancellation(ctx context.Context, cancelErr error) error {
	slog.Info(fmt.Sprintf("ğŸš« [å®¢æˆ·ç«¯å–æ¶ˆ] [%s] æ£€æµ‹åˆ°å®¢æˆ·ç«¯å–æ¶ˆ: %v", sp.requestID, cancelErr))
	
	// ç­‰å¾…åå°è§£æå®Œæˆï¼Œä½†ä¸è¶…è¿‡è¶…æ—¶æ—¶é—´
	if finished := sp.waitForParsingWithTimeout(2 * time.Second); finished {
		// æˆåŠŸç­‰å¾…è§£æå®Œæˆï¼Œæ”¶é›†å¯ç”¨ä¿¡æ¯
		return sp.collectAvailableInfo(cancelErr, "cancelled_with_data")
	} else {
		// è¶…æ—¶æœªå®Œæˆï¼Œæ”¶é›†éƒ¨åˆ†ä¿¡æ¯
		return sp.collectAvailableInfo(cancelErr, "cancelled_timeout")
	}
}

// waitForParsingWithTimeout å¸¦è¶…æ—¶çš„ç­‰å¾…è§£æå®Œæˆ - Phase 2 è¶…æ—¶ç­‰å¾…æœºåˆ¶
func (sp *StreamProcessor) waitForParsingWithTimeout(timeout time.Duration) bool {
	done := make(chan struct{})
	
	go func() {
		sp.parseWg.Wait()
		close(done)
	}()
	
	select {
	case <-done:
		slog.Info(fmt.Sprintf("âœ… [è§£æå®Œæˆ] [%s] åå°è§£æåœ¨å–æ¶ˆå‰å®Œæˆ", sp.requestID))
		return true
	case <-time.After(timeout):
		slog.Warn(fmt.Sprintf("â° [è§£æè¶…æ—¶] [%s] åå°è§£æåœ¨ %v å†…æœªå®Œæˆ", sp.requestID, timeout))
		return false
	}
}

// collectAvailableInfo æ™ºèƒ½ä¿¡æ¯æ”¶é›† - Phase 2 åˆ†é˜¶æ®µä¿å­˜é€»è¾‘  
func (sp *StreamProcessor) collectAvailableInfo(cancelErr error, status string) error {
	sp.parseMutex.Lock()
	defer sp.parseMutex.Unlock()
	
	// è®°å½•å–æ¶ˆæ—¶é—´
	duration := time.Since(sp.startTime)
	
	// å°è¯•è·å–å·²è§£æçš„ä¿¡æ¯
	modelName := sp.tokenParser.GetModelName()
	finalUsage := sp.tokenParser.GetFinalUsage()
	
	if sp.usageTracker != nil && sp.requestID != "" && !sp.completionRecorded {
		if finalUsage != nil && modelName != "" {
			// æœ‰å®Œæ•´Tokenä¿¡æ¯çš„å–æ¶ˆ
			sp.usageTracker.RecordRequestComplete(sp.requestID, modelName, finalUsage, duration)
			slog.Info(fmt.Sprintf("ğŸ’¾ [å®Œæ•´å–æ¶ˆ] [%s] ä¿å­˜å®Œæ•´Tokenä¿¡æ¯ - æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d", 
				sp.requestID, modelName, finalUsage.InputTokens, finalUsage.OutputTokens))
		} else if modelName != "" {
			// æœ‰æ¨¡å‹ä¿¡æ¯ä½†æ— Tokençš„å–æ¶ˆ
			emptyTokens := &tracking.TokenUsage{
				InputTokens: 0, OutputTokens: 0, 
				CacheCreationTokens: 0, CacheReadTokens: 0,
			}
			sp.usageTracker.RecordRequestComplete(sp.requestID, modelName+"_cancelled", emptyTokens, duration)
			slog.Info(fmt.Sprintf("ğŸ“ [éƒ¨åˆ†å–æ¶ˆ] [%s] ä¿å­˜æ¨¡å‹ä¿¡æ¯ - æ¨¡å‹: %s (å·²å–æ¶ˆ)", 
				sp.requestID, modelName))
		} else {
			// æ— ä»»ä½•ä¿¡æ¯çš„å–æ¶ˆ
			emptyTokens := &tracking.TokenUsage{
				InputTokens: 0, OutputTokens: 0, 
				CacheCreationTokens: 0, CacheReadTokens: 0,
			}
			sp.usageTracker.RecordRequestComplete(sp.requestID, "cancelled", emptyTokens, duration)
			slog.Info(fmt.Sprintf("ğŸš« [çº¯å–æ¶ˆ] [%s] å®¢æˆ·ç«¯åœ¨è¿æ¥å»ºç«‹åå–æ¶ˆ", sp.requestID))
		}
		
		// æ›´æ–°è¯·æ±‚çŠ¶æ€ä¸ºå–æ¶ˆ
		sp.usageTracker.RecordRequestUpdate(sp.requestID, sp.endpoint, "", status, 0, 0)
		sp.completionRecorded = true
	}
	
	return cancelErr
}