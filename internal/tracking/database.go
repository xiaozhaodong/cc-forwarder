package tracking

import (
	"context"
	"database/sql"
	"fmt"
	"log/slog"
	"strings"
	"time"
)

// initDatabase åˆå§‹åŒ–æ•°æ®åº“
func (ut *UsageTracker) initDatabase() error {
	// ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–æ•°æ®åº“schema
	if err := ut.adapter.InitSchema(); err != nil {
		return fmt.Errorf("failed to initialize database schema: %w", err)
	}

	slog.Debug("Database schema initialized successfully")
	return nil
}

// processEvents å¼‚æ­¥äº‹ä»¶å¤„ç†å¾ªç¯
func (ut *UsageTracker) processEvents() {
	defer ut.wg.Done()
	
	ticker := time.NewTicker(ut.config.FlushInterval)
	defer ticker.Stop()

	batch := make([]RequestEvent, 0, ut.config.BatchSize)

	slog.Debug("Usage tracking event processor started")

	for {
		select {
		case event := <-ut.eventChan:
			batch = append(batch, event)
			if len(batch) >= ut.config.BatchSize {
				ut.flushBatch(batch)
				batch = batch[:0] // é‡ç½®åˆ‡ç‰‡ä½†ä¿ç•™å®¹é‡
			}

		case <-ticker.C:
			if len(batch) > 0 {
				ut.flushBatch(batch)
				batch = batch[:0]
			}

		case <-ut.ctx.Done():
			// ä¼˜é›…å…³é—­ï¼Œå¤„ç†å‰©ä½™äº‹ä»¶
			slog.Debug("Processing remaining events before shutdown", "count", len(batch))
			if len(batch) > 0 {
				ut.flushBatch(batch)
			}
			
			// å¤„ç†é€šé“ä¸­å‰©ä½™çš„äº‹ä»¶
			for {
				select {
				case event := <-ut.eventChan:
					batch = append(batch, event)
					if len(batch) >= ut.config.BatchSize {
						ut.flushBatch(batch)
						batch = batch[:0]
					}
				default:
					if len(batch) > 0 {
						ut.flushBatch(batch)
					}
					slog.Debug("Usage tracking event processor stopped")
					return
				}
			}
		}
	}
}

// flushBatch æ‰¹é‡å†™å…¥äº‹ä»¶åˆ°æ•°æ®åº“
func (ut *UsageTracker) flushBatch(events []RequestEvent) {
	if len(events) == 0 {
		return
	}

	var retryCount int
	for retryCount < ut.config.MaxRetry {
		if err := ut.processBatch(events); err != nil {
			// ä½¿ç”¨é”™è¯¯å¤„ç†å™¨å¤„ç†é”™è¯¯
			if ut.errorHandler != nil && ut.errorHandler.HandleDatabaseError(err, "flushBatch") {
				// é”™è¯¯å·²å¤„ç†ï¼Œé‡è¯•æ“ä½œ
				retryCount++
				slog.Info("Database error handled successfully, retrying", 
					"retry", retryCount, 
					"batch_size", len(events))
				time.Sleep(time.Duration(retryCount) * time.Second)
				continue
			}
			
			retryCount++
			slog.Warn("Failed to flush batch, retrying", 
				"error", err, 
				"retry", retryCount, 
				"max_retry", ut.config.MaxRetry,
				"batch_size", len(events))
			
			if retryCount < ut.config.MaxRetry {
				time.Sleep(time.Duration(retryCount) * time.Second)
			}
			continue
		}
		
		// æˆåŠŸå¤„ç†
		if retryCount > 0 {
			slog.Info("Batch processed successfully after retry", 
				"retry_count", retryCount, 
				"batch_size", len(events))
		}
		return
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
	slog.Error("Failed to process batch after all retries", 
		"batch_size", len(events), 
		"max_retry", ut.config.MaxRetry)
}

// processBatch å¤„ç†ä¸€æ‰¹äº‹ä»¶ï¼ˆé‡æ„ä¸ºä½¿ç”¨å†™é˜Ÿåˆ—ï¼‰
func (ut *UsageTracker) processBatch(events []RequestEvent) error {
	successCount := 0
	
	for _, event := range events {
		// ç‰¹æ®Šå¤„ç†flushäº‹ä»¶
		if event.Type == "flush" {
			// flushäº‹ä»¶ä¸éœ€è¦å¤„ç†ï¼Œåªæ˜¯ç”¨æ¥è§¦å‘æ‰¹å¤„ç†
			successCount++
			continue
		}
		
		// æ„å»ºå†™æ“ä½œè¯·æ±‚
		query, args, err := ut.buildWriteQuery(event)
		if err != nil {
			slog.Error("Failed to build write query", 
				"error", err, 
				"event_type", event.Type, 
				"request_id", event.RequestID)
			continue
		}
		
		writeReq := WriteRequest{
			Query:     query,
			Args:      args,
			Response:  make(chan error, 1),
			Context:   context.Background(),
			EventType: event.Type,
		}
		
		// é€šè¿‡é˜Ÿåˆ—å‘é€å†™æ“ä½œ
		select {
		case ut.writeQueue <- writeReq:
			err := <-writeReq.Response
			if err != nil {
				slog.Error("Write operation failed", 
					"error", err, 
					"event_type", event.Type, 
					"request_id", event.RequestID)
				continue
			}
			successCount++
			
		case <-ut.ctx.Done():
			return ut.ctx.Err()
		}
	}

	if successCount < len(events) {
		slog.Warn("Some events failed to process", 
			"success", successCount, 
			"total", len(events))
	}

	return nil
}

// buildWriteQuery æ„å»ºå†™æ“ä½œæŸ¥è¯¢å’Œå‚æ•°
func (ut *UsageTracker) buildWriteQuery(event RequestEvent) (string, []interface{}, error) {
	switch event.Type {
	case "start":
		return ut.buildStartQuery(event)
	case "update":
		return ut.buildUpdateQuery(event)
	case "flexible_update": // æ–°å¢ï¼šç»Ÿä¸€çš„å¯é€‰å­—æ®µæ›´æ–°
		return ut.buildFlexibleUpdateQuery(event)
	case "success": // æ–°å¢ï¼šæˆåŠŸå®Œæˆï¼Œæ›¿ä»£"complete"
		return ut.buildSuccessQuery(event)
	case "final_failure": // æ–°å¢ï¼šå¤±è´¥/å–æ¶ˆå®Œæˆ
		return ut.buildFinalFailureQuery(event)
	case "complete":
		// å¯¹äºcompleteäº‹ä»¶ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æŒç»­æ—¶é—´ï¼Œä¸éœ€è¦æŸ¥è¯¢æ•°æ®åº“
		data, ok := event.Data.(RequestCompleteData)
		if !ok {
			return "", nil, fmt.Errorf("invalid complete event data type")
		}
		
		// è®¡ç®—æˆæœ¬
		tokens := &TokenUsage{
			InputTokens:         data.InputTokens,
			OutputTokens:        data.OutputTokens,
			CacheCreationTokens: data.CacheCreationTokens,
			CacheReadTokens:     data.CacheReadTokens,
		}
		
		inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)
		
		query := fmt.Sprintf(`UPDATE request_logs SET
			end_time = ?,
			duration_ms = ?,
			model_name = ?,
			input_tokens = ?,
			output_tokens = ?,
			cache_creation_tokens = ?,
			cache_read_tokens = ?,
			input_cost_usd = ?,
			output_cost_usd = ?,
			cache_creation_cost_usd = ?,
			cache_read_cost_usd = ?,
			total_cost_usd = ?,
			status = CASE WHEN status != 'completed' THEN 'completed' ELSE status END,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())
		
		args := []interface{}{
			event.Timestamp,
			data.Duration.Milliseconds(), // ç›´æ¥ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨è®¡ç®—çš„æŒç»­æ—¶é—´
			data.ModelName,
			data.InputTokens,
			data.OutputTokens,
			data.CacheCreationTokens,
			data.CacheReadTokens,
			inputCost,
			outputCost,
			cacheCost,
			readCost,
			totalCost,
			event.RequestID,
		}

		return query, args, nil
	case "failed_request_tokens":
		// å¤„ç†å¤±è´¥è¯·æ±‚Tokenäº‹ä»¶ï¼šåªè®°å½•Tokenç»Ÿè®¡ï¼Œä¸æ›´æ–°è¯·æ±‚çŠ¶æ€
		data, ok := event.Data.(RequestCompleteData)
		if !ok {
			return "", nil, fmt.Errorf("invalid failed_request_tokens event data type")
		}

		// è®¡ç®—æˆæœ¬
		tokens := &TokenUsage{
			InputTokens:         data.InputTokens,
			OutputTokens:        data.OutputTokens,
			CacheCreationTokens: data.CacheCreationTokens,
			CacheReadTokens:     data.CacheReadTokens,
		}

		inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)

		// åªæ›´æ–°Tokenç›¸å…³å­—æ®µå’Œæˆæœ¬ï¼Œä¸æ›´æ–°çŠ¶æ€
		// é‡è¦ï¼šåªæ›´æ–°å¤±è´¥çŠ¶æ€çš„è¯·æ±‚ï¼Œç¡®ä¿ä¸ä¼šå½±å“å·²å®Œæˆçš„è¯·æ±‚
		query := fmt.Sprintf(`UPDATE request_logs SET
			model_name = COALESCE(?, model_name),
			input_tokens = ?,
			output_tokens = ?,
			cache_creation_tokens = ?,
			cache_read_tokens = ?,
			input_cost_usd = ?,
			output_cost_usd = ?,
			cache_creation_cost_usd = ?,
			cache_read_cost_usd = ?,
			total_cost_usd = ?,
			duration_ms = COALESCE(?, duration_ms),
			updated_at = %s
		WHERE request_id = ?
		AND status IN ('error', 'timeout', 'suspended', 'cancelled', 'network_error', 'auth_error', 'rate_limited', 'stream_error')`, ut.adapter.BuildDateTimeNow())

		args := []interface{}{
			data.ModelName,
			data.InputTokens,
			data.OutputTokens,
			data.CacheCreationTokens,
			data.CacheReadTokens,
			inputCost,
			outputCost,
			cacheCost,
			readCost,
			totalCost,
			data.Duration.Milliseconds(),
			event.RequestID,
		}

		return query, args, nil
	case "token_recovery":
		// ğŸ”§ [Fallbackä¿®å¤] å¤„ç†Tokenæ¢å¤äº‹ä»¶ï¼šåªæ›´æ–°Tokenå­—æ®µå’Œæˆæœ¬ï¼Œä¸æ›´æ–°çŠ¶æ€
		data, ok := event.Data.(RequestCompleteData)
		if !ok {
			return "", nil, fmt.Errorf("invalid token_recovery event data type")
		}

		// è®¡ç®—æˆæœ¬
		tokens := &TokenUsage{
			InputTokens:         data.InputTokens,
			OutputTokens:        data.OutputTokens,
			CacheCreationTokens: data.CacheCreationTokens,
			CacheReadTokens:     data.CacheReadTokens,
		}

		inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)

		// ğŸ”§ ä¸“ç”¨äºæ¢å¤åœºæ™¯ï¼šæ›´æ–°ä»»ä½•çŠ¶æ€çš„è¯·æ±‚çš„Tokenå­—æ®µï¼Œå› ä¸ºè¿™æ˜¯æ¢å¤ä¸å®Œæ•´çš„æ•°æ®
		query := fmt.Sprintf(`UPDATE request_logs SET
			model_name = COALESCE(?, model_name),
			input_tokens = ?,
			output_tokens = ?,
			cache_creation_tokens = ?,
			cache_read_tokens = ?,
			input_cost_usd = ?,
			output_cost_usd = ?,
			cache_creation_cost_usd = ?,
			cache_read_cost_usd = ?,
			total_cost_usd = ?,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

		args := []interface{}{
			data.ModelName,
			data.InputTokens,
			data.OutputTokens,
			data.CacheCreationTokens,
			data.CacheReadTokens,
			inputCost,
			outputCost,
			cacheCost,
			readCost,
			totalCost,
			event.RequestID,
		}

		return query, args, nil
	default:
		return "", nil, fmt.Errorf("unknown event type: %s", event.Type)
	}
}

// buildStartQuery æ„å»ºå¼€å§‹äº‹ä»¶æŸ¥è¯¢
func (ut *UsageTracker) buildStartQuery(event RequestEvent) (string, []interface{}, error) {
	data, ok := event.Data.(RequestStartData)
	if !ok {
		return "", nil, fmt.Errorf("invalid start event data type")
	}

	// ä½¿ç”¨é€‚é…å™¨æ„å»ºINSERT OR REPLACEæŸ¥è¯¢
	columns := []string{"request_id", "client_ip", "user_agent", "method", "path", "start_time", "status", "is_streaming", "updated_at"}
	placeholders := []string{"?", "?", "?", "?", "?", "?", "'pending'", "?", ut.adapter.BuildDateTimeNow()}

	query := ut.adapter.BuildInsertOrReplaceQuery("request_logs", columns, placeholders)

	args := []interface{}{
		event.RequestID,
		data.ClientIP,
		data.UserAgent,
		data.Method,
		data.Path,
		event.Timestamp,
		data.IsStreaming,
	}

	return query, args, nil
}

// buildUpdateQuery æ„å»ºæ›´æ–°äº‹ä»¶æŸ¥è¯¢
func (ut *UsageTracker) buildUpdateQuery(event RequestEvent) (string, []interface{}, error) {
	data, ok := event.Data.(RequestUpdateData)
	if !ok {
		return "", nil, fmt.Errorf("invalid update event data type")
	}

	// å¦‚æœç«¯ç‚¹åå’Œç»„åéƒ½ä¸ºç©ºï¼Œåªæ›´æ–°çŠ¶æ€ç›¸å…³å­—æ®µ
	if data.EndpointName == "" && data.GroupName == "" {
		query := fmt.Sprintf(`UPDATE request_logs SET
			status = ?,
			retry_count = ?,
			http_status_code = ?,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

		args := []interface{}{
			data.Status,
			data.RetryCount,
			data.HTTPStatus,
			event.RequestID,
		}

		return query, args, nil
	}

	// ä½¿ç”¨é€‚é…å™¨æ„å»ºINSERT OR REPLACEæŸ¥è¯¢
	// é‡æ–°åŠ å…¥start_timeï¼Œä½†åœ¨UPSERTæ—¶ä¿æŠ¤å·²æœ‰å€¼ä¸è¢«è¦†ç›–
	columns := []string{"request_id", "endpoint_name", "group_name", "status", "retry_count", "http_status_code", "start_time", "updated_at"}
	placeholders := []string{"?", "?", "?", "?", "?", "?", "?", ut.adapter.BuildDateTimeNow()}
	query := ut.adapter.BuildInsertOrReplaceQuery("request_logs", columns, placeholders)

	args := []interface{}{
		event.RequestID,
		data.EndpointName,
		data.GroupName,
		data.Status,
		data.RetryCount,
		data.HTTPStatus,
		event.Timestamp, // æä¾›start_timeå€¼ç”¨äºæ’å…¥æ–°è®°å½•
	}

	return query, args, nil
}

// buildFlexibleUpdateQuery æ„å»ºç»Ÿä¸€çš„å¯é€‰å­—æ®µæ›´æ–°æŸ¥è¯¢
// æ”¯æŒå¯é€‰å­—æ®µæ›´æ–°ï¼Œåªæ›´æ–°énilçš„å­—æ®µ
func (ut *UsageTracker) buildFlexibleUpdateQuery(event RequestEvent) (string, []interface{}, error) {
	opts, ok := event.Data.(UpdateOptions)
	if !ok {
		return "", nil, fmt.Errorf("invalid flexible_update event data type")
	}

	// æ„å»ºåŠ¨æ€UPDATEè¯­å¥
	var setParts []string
	var args []interface{}

	// æ ¹æ®UpdateOptionsä¸­çš„énilå­—æ®µæ„å»ºSETå­å¥
	if opts.EndpointName != nil {
		setParts = append(setParts, "endpoint_name = ?")
		args = append(args, *opts.EndpointName)
	}
	if opts.GroupName != nil {
		setParts = append(setParts, "group_name = ?")
		args = append(args, *opts.GroupName)
	}
	if opts.Status != nil {
		setParts = append(setParts, "status = ?")
		args = append(args, *opts.Status)
	}
	if opts.RetryCount != nil {
		setParts = append(setParts, "retry_count = ?")
		args = append(args, *opts.RetryCount)
	}
	if opts.HttpStatus != nil {
		setParts = append(setParts, "http_status_code = ?")
		args = append(args, *opts.HttpStatus)
	}
	if opts.ModelName != nil {
		setParts = append(setParts, "model_name = ?")
		args = append(args, *opts.ModelName)
	}
	if opts.EndTime != nil {
		setParts = append(setParts, "end_time = ?")
		args = append(args, *opts.EndTime)
	}
	if opts.Duration != nil {
		setParts = append(setParts, "duration_ms = ?")
		args = append(args, opts.Duration.Milliseconds())
	}
	if opts.FailureReason != nil {
		setParts = append(setParts, "failure_reason = ?")
		args = append(args, *opts.FailureReason)
	}

	// å¦‚æœæ²¡æœ‰å­—æ®µéœ€è¦æ›´æ–°ï¼Œè¿”å›é”™è¯¯
	if len(setParts) == 0 {
		return "", nil, fmt.Errorf("no fields to update in flexible_update event")
	}

	// æ€»æ˜¯æ›´æ–°updated_atå­—æ®µ
	setParts = append(setParts, fmt.Sprintf("updated_at = %s", ut.adapter.BuildDateTimeNow()))

	// æ„å»ºå®Œæ•´çš„UPDATEè¯­å¥
	query := fmt.Sprintf("UPDATE request_logs SET %s WHERE request_id = ?",
		strings.Join(setParts, ", "))

	// æ·»åŠ WHEREæ¡ä»¶çš„å‚æ•°
	args = append(args, event.RequestID)

	return query, args, nil
}

// buildSuccessQuery æ„å»ºæˆåŠŸå®Œæˆçš„æŸ¥è¯¢
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æˆåŠŸç›¸å…³å­—æ®µï¼šstatus='completed', end_time, duration_ms, Tokenå’Œæˆæœ¬ä¿¡æ¯
func (ut *UsageTracker) buildSuccessQuery(event RequestEvent) (string, []interface{}, error) {
	data, ok := event.Data.(RequestCompleteData)
	if !ok {
		return "", nil, fmt.Errorf("invalid success event data type")
	}

	// è®¡ç®—æˆæœ¬
	tokens := &TokenUsage{
		InputTokens:         data.InputTokens,
		OutputTokens:        data.OutputTokens,
		CacheCreationTokens: data.CacheCreationTokens,
		CacheReadTokens:     data.CacheReadTokens,
	}

	inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)

	query := fmt.Sprintf(`UPDATE request_logs SET
		end_time = ?,
		duration_ms = ?,
		model_name = ?,
		input_tokens = ?,
		output_tokens = ?,
		cache_creation_tokens = ?,
		cache_read_tokens = ?,
		input_cost_usd = ?,
		output_cost_usd = ?,
		cache_creation_cost_usd = ?,
		cache_read_cost_usd = ?,
		total_cost_usd = ?,
		http_status_code = CASE WHEN http_status_code IS NULL OR http_status_code = 0 THEN 200 ELSE http_status_code END,
		status = 'completed',
		updated_at = %s
	WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

	args := []interface{}{
		event.Timestamp,
		data.Duration.Milliseconds(),
		data.ModelName,
		data.InputTokens,
		data.OutputTokens,
		data.CacheCreationTokens,
		data.CacheReadTokens,
		inputCost,
		outputCost,
		cacheCost,
		readCost,
		totalCost,
		event.RequestID,
	}

	return query, args, nil
}

// buildFinalFailureQuery æ„å»ºå¤±è´¥/å–æ¶ˆå®Œæˆçš„æŸ¥è¯¢
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰å¤±è´¥/å–æ¶ˆç›¸å…³å­—æ®µï¼šstatus, end_time, duration_ms, failure_reason/cancel_reason, å¯é€‰Token
func (ut *UsageTracker) buildFinalFailureQuery(event RequestEvent) (string, []interface{}, error) {
	data, ok := event.Data.(map[string]interface{})
	if !ok {
		return "", nil, fmt.Errorf("invalid final_failure event data type")
	}

	status, _ := data["status"].(string)
	reason, _ := data["reason"].(string)
	errorDetail, _ := data["error_detail"].(string)
	duration, _ := data["duration"].(time.Duration)
	httpStatus, _ := data["http_status"].(int)
	inputTokens, _ := data["input_tokens"].(int64)
	outputTokens, _ := data["output_tokens"].(int64)
	cacheCreationTokens, _ := data["cache_creation_tokens"].(int64)
	cacheReadTokens, _ := data["cache_read_tokens"].(int64)

	// æ ¹æ®çŠ¶æ€è®¾ç½®ç›¸åº”çš„reasonå­—æ®µ
	var query string
	var args []interface{}

	if status == "cancelled" {
		query = fmt.Sprintf(`UPDATE request_logs SET
			end_time = ?,
			duration_ms = ?,
			status = 'cancelled',
			cancel_reason = ?,
			http_status_code = ?,
			input_tokens = ?,
			output_tokens = ?,
			cache_creation_tokens = ?,
			cache_read_tokens = ?,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

		args = []interface{}{
			event.Timestamp,
			duration.Milliseconds(),
			reason, // cancel_reason
			httpStatus, // http_status_code
			inputTokens,
			outputTokens,
			cacheCreationTokens,
			cacheReadTokens,
			event.RequestID,
		}
	} else {
		// status == "failed"
		query = fmt.Sprintf(`UPDATE request_logs SET
			end_time = ?,
			duration_ms = ?,
			status = 'failed',
			failure_reason = ?,
			last_failure_reason = ?,
			http_status_code = ?,
			input_tokens = ?,
			output_tokens = ?,
			cache_creation_tokens = ?,
			cache_read_tokens = ?,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

		args = []interface{}{
			event.Timestamp,
			duration.Milliseconds(),
			reason,      // failure_reason
			errorDetail, // last_failure_reason
			httpStatus,  // http_status_code
			inputTokens,
			outputTokens,
			cacheCreationTokens,
			cacheReadTokens,
			event.RequestID,
		}
	}

	return query, args, nil
}

// buildCompleteQueryWithTx åœ¨äº‹åŠ¡ä¸­æ„å»ºå®Œæˆäº‹ä»¶æŸ¥è¯¢ï¼Œå¯ä»¥æŸ¥è¯¢start_timeè®¡ç®—å‡†ç¡®æŒç»­æ—¶é—´
func (ut *UsageTracker) buildCompleteQueryWithTx(ctx context.Context, tx *sql.Tx, event RequestEvent) (string, []interface{}, error) {
	data, ok := event.Data.(RequestCompleteData)
	if !ok {
		return "", nil, fmt.Errorf("invalid complete event data type")
	}

	// é¦–å…ˆè·å–è¯·æ±‚çš„å¼€å§‹æ—¶é—´æ¥è®¡ç®—æ­£ç¡®çš„æŒç»­æ—¶é—´
	var startTime time.Time
	var durationMs int64
	
	queryStartTime := `SELECT start_time FROM request_logs WHERE request_id = ?`
	err := tx.QueryRowContext(ctx, queryStartTime, event.RequestID).Scan(&startTime)
	if err != nil {
		if err == sql.ErrNoRows {
			// è®°å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼°ç®—çš„å¼€å§‹æ—¶é—´
			startTime = event.Timestamp.Add(-data.Duration)
			durationMs = data.Duration.Milliseconds()
		} else {
			return "", nil, fmt.Errorf("failed to get start time for duration calculation: %w", err)
		}
	} else {
		// è®¡ç®—æ­£ç¡®çš„æŒç»­æ—¶é—´ï¼šend_time - start_time
		durationMs = event.Timestamp.Sub(startTime).Milliseconds()
	}

	// è®¡ç®—æˆæœ¬
	tokens := &TokenUsage{
		InputTokens:         data.InputTokens,
		OutputTokens:        data.OutputTokens,
		CacheCreationTokens: data.CacheCreationTokens,
		CacheReadTokens:     data.CacheReadTokens,
	}

	inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)

	// ä½¿ç”¨è®¡ç®—å‡ºçš„å‡†ç¡®æŒç»­æ—¶é—´
	query := fmt.Sprintf(`UPDATE request_logs SET
		end_time = ?,
		duration_ms = ?,
		model_name = ?,
		input_tokens = ?,
		output_tokens = ?,
		cache_creation_tokens = ?,
		cache_read_tokens = ?,
		input_cost_usd = ?,
		output_cost_usd = ?,
		cache_creation_cost_usd = ?,
		cache_read_cost_usd = ?,
		total_cost_usd = ?,
		status = CASE WHEN status != 'completed' THEN 'completed' ELSE status END,
		updated_at = %s
	WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

	args := []interface{}{
		event.Timestamp,
		durationMs, // ä½¿ç”¨è®¡ç®—å‡ºçš„å‡†ç¡®æŒç»­æ—¶é—´
		data.ModelName,
		data.InputTokens,
		data.OutputTokens,
		data.CacheCreationTokens,
		data.CacheReadTokens,
		inputCost,
		outputCost,
		cacheCost,
		readCost,
		totalCost,
		event.RequestID,
	}

	return query, args, nil
}

// insertRequestStart æ’å…¥è¯·æ±‚å¼€å§‹è®°å½•
func (ut *UsageTracker) insertRequestStart(ctx context.Context, tx *sql.Tx, event RequestEvent) error {
	data, ok := event.Data.(RequestStartData)
	if !ok {
		return fmt.Errorf("invalid start event data type")
	}

	// ä½¿ç”¨é€‚é…å™¨æ„å»ºINSERT OR REPLACEæŸ¥è¯¢
	columns := []string{"request_id", "client_ip", "user_agent", "method", "path", "start_time", "status", "is_streaming", "updated_at"}
	placeholders := []string{"?", "?", "?", "?", "?", "?", "'pending'", "?", ut.adapter.BuildDateTimeNow()}
	query := ut.adapter.BuildInsertOrReplaceQuery("request_logs", columns, placeholders)

	_, err := tx.ExecContext(ctx, query,
		event.RequestID,
		data.ClientIP,
		data.UserAgent,
		data.Method,
		data.Path,
		event.Timestamp,
		data.IsStreaming)

	return err
}

// updateRequestStatus æ›´æ–°è¯·æ±‚çŠ¶æ€
func (ut *UsageTracker) updateRequestStatus(ctx context.Context, tx *sql.Tx, event RequestEvent) error {
	data, ok := event.Data.(RequestUpdateData)
	if !ok {
		return fmt.Errorf("invalid update event data type")
	}

	// å¦‚æœç«¯ç‚¹åå’Œç»„åéƒ½ä¸ºç©ºï¼Œåªæ›´æ–°çŠ¶æ€ç›¸å…³å­—æ®µ
	if data.EndpointName == "" && data.GroupName == "" {
		query := fmt.Sprintf(`UPDATE request_logs SET
			status = ?,
			retry_count = ?,
			http_status_code = ?,
			updated_at = %s
		WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

		result, err := tx.ExecContext(ctx, query,
			data.Status,
			data.RetryCount,
			data.HTTPStatus,
			event.RequestID)
		
		if err != nil {
			return err
		}

		// æ£€æŸ¥æ˜¯å¦æ›´æ–°äº†è®°å½•
		rowsAffected, err := result.RowsAffected()
		if err != nil {
			return err
		}
		
		if rowsAffected == 0 {
			// è®°å½•ä¸å­˜åœ¨ï¼Œå…ˆåˆ›å»ºä¸€ä¸ªåŸºæœ¬è®°å½•
			return ut.insertRequestStart(ctx, tx, RequestEvent{
				Type:      "start", 
				RequestID: event.RequestID,
				Timestamp: event.Timestamp,
				Data: RequestStartData{
					ClientIP:    "unknown",
					UserAgent:   "unknown", 
					Method:      "unknown",
					Path:        "unknown",
					IsStreaming: false, // é»˜è®¤ä¸ºéæµå¼
				},
			})
		}
		
		return nil
	}

	// æ­£å¸¸æ›´æ–°æ‰€æœ‰å­—æ®µ
	query := fmt.Sprintf(`UPDATE request_logs SET
		endpoint_name = ?,
		group_name = ?,
		status = ?,
		retry_count = ?,
		http_status_code = ?,
		updated_at = %s
	WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

	result, err := tx.ExecContext(ctx, query,
		data.EndpointName,
		data.GroupName,
		data.Status,
		data.RetryCount,
		data.HTTPStatus,
		event.RequestID)
	
	if err != nil {
		return err
	}

	// æ£€æŸ¥æ˜¯å¦æ›´æ–°äº†è®°å½•
	rowsAffected, err := result.RowsAffected()
	if err != nil {
		return err
	}
	
	if rowsAffected == 0 {
		// è®°å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨é€‚é…å™¨æ„å»ºINSERT OR REPLACEæŸ¥è¯¢
		columns := []string{"request_id", "endpoint_name", "group_name", "status", "retry_count", "http_status_code", "start_time", "updated_at"}
		placeholders := []string{"?", "?", "?", "?", "?", "?", "?", ut.adapter.BuildDateTimeNow()}
		insertQuery := ut.adapter.BuildInsertOrReplaceQuery("request_logs", columns, placeholders)

		_, err = tx.ExecContext(ctx, insertQuery,
			event.RequestID,
			data.EndpointName,
			data.GroupName,
			data.Status,
			data.RetryCount,
			data.HTTPStatus,
			event.Timestamp)
	}

	return err
}

// completeRequest å®Œæˆè¯·æ±‚è®°å½•
func (ut *UsageTracker) completeRequest(ctx context.Context, tx *sql.Tx, event RequestEvent) error {
	data, ok := event.Data.(RequestCompleteData)
	if !ok {
		return fmt.Errorf("invalid complete event data type")
	}

	// é¦–å…ˆè·å–è¯·æ±‚çš„å¼€å§‹æ—¶é—´æ¥è®¡ç®—æ­£ç¡®çš„æŒç»­æ—¶é—´
	var startTime time.Time
	var durationMs int64
	
	queryStartTime := `SELECT start_time FROM request_logs WHERE request_id = ?`
	err := tx.QueryRowContext(ctx, queryStartTime, event.RequestID).Scan(&startTime)
	if err != nil {
		if err == sql.ErrNoRows {
			// è®°å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¼°ç®—çš„å¼€å§‹æ—¶é—´
			startTime = event.Timestamp.Add(-data.Duration)
			durationMs = data.Duration.Milliseconds()
		} else {
			return fmt.Errorf("failed to get start time for duration calculation: %w", err)
		}
	} else {
		// è®¡ç®—æ­£ç¡®çš„æŒç»­æ—¶é—´ï¼šend_time - start_time
		durationMs = event.Timestamp.Sub(startTime).Milliseconds()
	}

	// è®¡ç®—æˆæœ¬
	tokens := &TokenUsage{
		InputTokens:         data.InputTokens,
		OutputTokens:        data.OutputTokens,
		CacheCreationTokens: data.CacheCreationTokens,
		CacheReadTokens:     data.CacheReadTokens,
	}
	
	inputCost, outputCost, cacheCost, readCost, totalCost := ut.calculateCost(data.ModelName, tokens)

	query := fmt.Sprintf(`UPDATE request_logs SET
		end_time = ?,
		duration_ms = ?,
		model_name = ?,
		input_tokens = ?,
		output_tokens = ?,
		cache_creation_tokens = ?,
		cache_read_tokens = ?,
		input_cost_usd = ?,
		output_cost_usd = ?,
		cache_creation_cost_usd = ?,
		cache_read_cost_usd = ?,
		total_cost_usd = ?,
		status = CASE WHEN status != 'completed' THEN 'completed' ELSE status END,
		updated_at = %s
	WHERE request_id = ?`, ut.adapter.BuildDateTimeNow())

	result, err := tx.ExecContext(ctx, query,
		event.Timestamp,
		durationMs,  // ä½¿ç”¨è®¡ç®—å‡ºçš„æ­£ç¡®æŒç»­æ—¶é—´
		data.ModelName,
		data.InputTokens,
		data.OutputTokens,
		data.CacheCreationTokens,
		data.CacheReadTokens,
		inputCost,
		outputCost,
		cacheCost,
		readCost,
		totalCost,
		event.RequestID)

	if err != nil {
		return err
	}

	// æ£€æŸ¥æ˜¯å¦æ›´æ–°äº†è®°å½•
	rowsAffected, err := result.RowsAffected()
	if err != nil {
		return err
	}
	
	if rowsAffected == 0 {
		// è®°å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®Œæ•´è®°å½•
		insertQuery := `INSERT INTO request_logs (
			request_id, start_time, end_time, duration_ms, model_name,
			input_tokens, output_tokens, cache_creation_tokens, cache_read_tokens,
			input_cost_usd, output_cost_usd, cache_creation_cost_usd, 
			cache_read_cost_usd, total_cost_usd, status
		) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'completed')`
		
		// ä½¿ç”¨å·²è®¡ç®—çš„ startTime å’Œ durationMs
		_, err = tx.ExecContext(ctx, insertQuery,
			event.RequestID,
			startTime,
			event.Timestamp,
			durationMs,
			data.ModelName,
			data.InputTokens,
			data.OutputTokens,
			data.CacheCreationTokens,
			data.CacheReadTokens,
			inputCost,
			outputCost,
			cacheCost,
			readCost,
			totalCost)
	}

	return err
}

// calculateCost è®¡ç®—è¯·æ±‚æˆæœ¬
func (ut *UsageTracker) calculateCost(modelName string, tokens *TokenUsage) (inputCost, outputCost, cacheCost, readCost, totalCost float64) {
	pricing := ut.GetPricing(modelName)
	
	inputCost = float64(tokens.InputTokens) * pricing.Input / 1000000
	outputCost = float64(tokens.OutputTokens) * pricing.Output / 1000000
	cacheCost = float64(tokens.CacheCreationTokens) * pricing.CacheCreation / 1000000
	readCost = float64(tokens.CacheReadTokens) * pricing.CacheRead / 1000000
	totalCost = inputCost + outputCost + cacheCost + readCost

	return
}

// periodicCleanup å®šæœŸæ¸…ç†å†å²æ•°æ®
func (ut *UsageTracker) periodicCleanup() {
	defer ut.wg.Done()
	
	ticker := time.NewTicker(ut.config.CleanupInterval)
	defer ticker.Stop()

	slog.Debug("Periodic cleanup task started", "interval", ut.config.CleanupInterval)

	for {
		select {
		case <-ticker.C:
			if err := ut.cleanupOldRecords(); err != nil {
				slog.Error("Failed to cleanup old records", "error", err)
			}
			
		case <-ut.ctx.Done():
			slog.Debug("Periodic cleanup task stopped")
			return
		}
	}
}

// cleanupOldRecords æ¸…ç†è¿‡æœŸè®°å½•ï¼ˆä½¿ç”¨å†™é˜Ÿåˆ—ï¼‰
func (ut *UsageTracker) cleanupOldRecords() error {
	if ut.config.RetentionDays <= 0 {
		return nil // æ°¸ä¹…ä¿ç•™
	}

	cutoffTime := time.Now().AddDate(0, 0, -ut.config.RetentionDays)
	
	// åˆ é™¤è¿‡æœŸçš„è¯·æ±‚è®°å½•ï¼ˆé€šè¿‡å†™é˜Ÿåˆ—ï¼‰
	requestQuery := "DELETE FROM request_logs WHERE start_time < ?"
	requestWriteReq := WriteRequest{
		Query:     requestQuery,
		Args:      []interface{}{cutoffTime},
		Response:  make(chan error, 1),
		Context:   context.Background(),
		EventType: "cleanup_requests",
	}
	
	select {
	case ut.writeQueue <- requestWriteReq:
		err := <-requestWriteReq.Response
		if err != nil {
			return fmt.Errorf("failed to delete old request logs: %w", err)
		}
	case <-ut.ctx.Done():
		return ut.ctx.Err()
	}
	
	// æ¸…ç†è¿‡æœŸçš„æ±‡æ€»æ•°æ®ï¼ˆé€šè¿‡å†™é˜Ÿåˆ—ï¼‰
	summaryQuery := "DELETE FROM usage_summary WHERE date < ?"
	summaryWriteReq := WriteRequest{
		Query:     summaryQuery,
		Args:      []interface{}{cutoffTime.Format("2006-01-02")},
		Response:  make(chan error, 1),
		Context:   context.Background(),
		EventType: "cleanup_summaries",
	}
	
	select {
	case ut.writeQueue <- summaryWriteReq:
		err := <-summaryWriteReq.Response
		if err != nil {
			return fmt.Errorf("failed to delete old usage summary: %w", err)
		}
	case <-ut.ctx.Done():
		return ut.ctx.Err()
	}
	
	// è¿è¡ŒVACUUMä»¥å›æ”¶ç©ºé—´ï¼ˆé€šè¿‡å†™é˜Ÿåˆ—ï¼Œä»…å¯¹SQLiteæœ‰æ•ˆï¼‰
	if ut.adapter.GetDatabaseType() == "sqlite" {
		vacuumWriteReq := WriteRequest{
			Query:     "VACUUM",
			Args:      []interface{}{},
			Response:  make(chan error, 1),
			Context:   context.Background(),
			EventType: "vacuum",
		}

		select {
		case ut.writeQueue <- vacuumWriteReq:
			err := <-vacuumWriteReq.Response
			if err != nil {
				slog.Warn("Failed to vacuum database after cleanup", "error", err)
			}
		case <-ut.ctx.Done():
			return ut.ctx.Err()
		}
	}

	// è®°å½•æ¸…ç†ç»“æœ
	slog.Info("Cleaned up old records", 
		"cutoff_date", cutoffTime.Format("2006-01-02"),
		"retention_days", ut.config.RetentionDays)
	
	// æ›´æ–°æ±‡æ€»ç»Ÿè®¡ï¼ˆå¼‚æ­¥ï¼‰
	go ut.updateUsageSummary()

	return nil
}

// vacuumDatabase è¿è¡ŒVACUUMå‘½ä»¤å›æ”¶æ•°æ®åº“ç©ºé—´ï¼ˆä»…SQLiteéœ€è¦ï¼‰
func (ut *UsageTracker) vacuumDatabase(ctx context.Context) error {
	if ut.adapter.GetDatabaseType() != "sqlite" {
		slog.Debug("Skipping VACUUM - not a SQLite database")
		return nil
	}

	slog.Debug("Running database VACUUM to reclaim space...")

	// VACUUMä¸èƒ½åœ¨äº‹åŠ¡ä¸­è¿è¡Œ
	_, err := ut.db.ExecContext(ctx, "VACUUM")
	if err != nil {
		return fmt.Errorf("failed to vacuum database: %w", err)
	}

	slog.Debug("Database VACUUM completed successfully")
	return nil
}

// updateUsageSummary æ›´æ–°ä½¿ç”¨æ±‡æ€»æ•°æ®ï¼ˆä½¿ç”¨å†™é˜Ÿåˆ—ï¼‰
func (ut *UsageTracker) updateUsageSummary() {
	// è·å–éœ€è¦æ›´æ–°æ±‡æ€»çš„æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘7å¤©ï¼‰
	endDate := time.Now()
	startDate := endDate.AddDate(0, 0, -7)

	// ä½¿ç”¨é€‚é…å™¨æ„å»ºINSERT OR REPLACEæŸ¥è¯¢
	columns := []string{
		"date", "model_name", "endpoint_name", "group_name",
		"request_count", "success_count", "error_count",
		"total_input_tokens", "total_output_tokens",
		"total_cache_creation_tokens", "total_cache_read_tokens",
		"total_cost_usd", "avg_duration_ms",
		"created_at", "updated_at",
	}

	placeholders := make([]string, len(columns))
	for i := range placeholders {
		placeholders[i] = "?"
	}

	baseQuery := ut.adapter.BuildInsertOrReplaceQuery("usage_summary", columns, placeholders)

	// æ„å»ºSELECTå­æŸ¥è¯¢
	selectQuery := fmt.Sprintf(`
	SELECT
		DATE(start_time) as date,
		COALESCE(model_name, '') as model_name,
		COALESCE(endpoint_name, '') as endpoint_name,
		COALESCE(group_name, '') as group_name,
		COUNT(*) as request_count,
		SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as success_count,
		SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as error_count,
		SUM(input_tokens) as total_input_tokens,
		SUM(output_tokens) as total_output_tokens,
		SUM(cache_creation_tokens) as total_cache_creation_tokens,
		SUM(cache_read_tokens) as total_cache_read_tokens,
		SUM(total_cost_usd) as total_cost_usd,
		AVG(CASE WHEN duration_ms IS NOT NULL AND duration_ms > 0 THEN duration_ms ELSE NULL END) as avg_duration_ms,
		%s as created_at,
		%s as updated_at
	FROM request_logs
	WHERE start_time >= ? AND start_time < ?
		AND (model_name IS NOT NULL OR endpoint_name IS NOT NULL)
	GROUP BY DATE(start_time), model_name, endpoint_name, group_name
	`, ut.adapter.BuildDateTimeNow(), ut.adapter.BuildDateTimeNow())

	// æ‹¼æ¥å®Œæ•´æŸ¥è¯¢
	query := strings.Replace(baseQuery, "VALUES ("+strings.Join(placeholders, ", ")+")", "("+selectQuery+")", 1)

	summaryWriteReq := WriteRequest{
		Query:     query,
		Args:      []interface{}{startDate, endDate.AddDate(0, 0, 1)},
		Response:  make(chan error, 1),
		Context:   context.Background(),
		EventType: "update_summary",
	}

	select {
	case ut.writeQueue <- summaryWriteReq:
		err := <-summaryWriteReq.Response
		if err != nil {
			slog.Error("Failed to update usage summary", "error", err)
		} else {
			slog.Info("Usage summary updated successfully")
		}
	case <-ut.ctx.Done():
		slog.Debug("Usage summary update cancelled due to context cancellation")
	}
}

// GetDatabaseStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) getDatabaseStatsInternal(ctx context.Context) (*DatabaseStats, error) {
	if ut.readDB == nil {
		return nil, fmt.Errorf("read database not initialized")
	}

	stats := &DatabaseStats{}
	
	// è·å–è¯·æ±‚è®°å½•æ€»æ•°ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	err := ut.readDB.QueryRowContext(ctx, "SELECT COUNT(*) FROM request_logs").Scan(&stats.TotalRequests)
	if err != nil {
		return nil, fmt.Errorf("failed to get total requests count: %w", err)
	}
	
	// è·å–æ±‡æ€»è®°å½•æ€»æ•°ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	err = ut.readDB.QueryRowContext(ctx, "SELECT COUNT(*) FROM usage_summary").Scan(&stats.TotalSummaries)
	if err != nil {
		return nil, fmt.Errorf("failed to get total summaries count: %w", err)
	}
	
	// è·å–æœ€æ—©å’Œæœ€æ–°çš„è®°å½•æ—¶é—´ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	var earliestStr, latestStr sql.NullString
	err = ut.readDB.QueryRowContext(ctx, "SELECT MIN(start_time), MAX(start_time) FROM request_logs").Scan(&earliestStr, &latestStr)
	if err != nil && err != sql.ErrNoRows {
		return nil, fmt.Errorf("failed to get record time range: %w", err)
	}
	
	if earliestStr.Valid {
		if t, err := time.Parse(time.RFC3339, earliestStr.String); err == nil {
			stats.EarliestRecord = &t
		}
	}
	if latestStr.Valid {
		if t, err := time.Parse(time.RFC3339, latestStr.String); err == nil {
			stats.LatestRecord = &t
		}
	}
	
	// è·å–æ•°æ®åº“æ–‡ä»¶å¤§å°ï¼ˆSQLiteç‰¹æœ‰ï¼Œä½¿ç”¨è¯»è¿æ¥ï¼‰
	if ut.adapter.GetDatabaseType() == "sqlite" {
		var pageCount, pageSize int64
		err = ut.readDB.QueryRowContext(ctx, "PRAGMA page_count").Scan(&pageCount)
		if err == nil {
			err = ut.readDB.QueryRowContext(ctx, "PRAGMA page_size").Scan(&pageSize)
			if err == nil {
				stats.DatabaseSize = pageCount * pageSize
			}
		}
	} else {
		// MySQLæ•°æ®åº“å¤§å°æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
		// è¿™é‡Œå¯ä»¥æ·»åŠ æŸ¥è¯¢information_schemaæ¥è·å–è¡¨å¤§å°çš„é€»è¾‘
		stats.DatabaseSize = 0
	}
	
	// è·å–æ€»æˆæœ¬ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	err = ut.readDB.QueryRowContext(ctx, "SELECT COALESCE(SUM(total_cost_usd), 0) FROM request_logs WHERE total_cost_usd > 0").Scan(&stats.TotalCostUSD)
	if err != nil && err != sql.ErrNoRows {
		return nil, fmt.Errorf("failed to get total cost: %w", err)
	}
	
	return stats, nil
}

// periodicBackup å®šæœŸå¤‡ä»½æ•°æ®åº“
func (ut *UsageTracker) periodicBackup() {
	defer ut.wg.Done()
	
	ticker := time.NewTicker(6 * time.Hour) // æ¯6å°æ—¶å¤‡ä»½ä¸€æ¬¡
	defer ticker.Stop()

	slog.Debug("Periodic backup task started")

	for {
		select {
		case <-ticker.C:
			if ut.errorHandler != nil {
				if err := ut.errorHandler.CreateBackup(); err != nil {
					slog.Error("Scheduled backup failed", "error", err)
				} else {
					slog.Debug("Scheduled backup completed successfully")
				}
			}
			
		case <-ut.ctx.Done():
			slog.Debug("Periodic backup task stopped")
			return
		}
	}
}

// DatabaseStats æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
type DatabaseStats struct {
	TotalRequests   int64      `json:"total_requests"`
	TotalSummaries  int64      `json:"total_summaries"`
	EarliestRecord  *time.Time `json:"earliest_record,omitempty"`
	LatestRecord    *time.Time `json:"latest_record,omitempty"`
	DatabaseSize    int64      `json:"database_size_bytes"`
	TotalCostUSD    float64    `json:"total_cost_usd"`
}