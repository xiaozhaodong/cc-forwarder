package tracking

import (
	"context"
	"database/sql"
	"embed"
	"encoding/json"
	"fmt"
	"log/slog"
	"sync"
	"time"

	"cc-forwarder/config"
	_ "modernc.org/sqlite"
)

//go:embed schema.sql
var schemaFS embed.FS

// UsageStatsDetailed è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡
type UsageStatsDetailed struct {
	TotalRequests    int64                      `json:"total_requests"`
	SuccessRequests  int64                      `json:"success_requests"`
	ErrorRequests    int64                      `json:"error_requests"`
	TotalTokens      int64                      `json:"total_tokens"`
	TotalCost        float64                    `json:"total_cost"`
	ModelStats       map[string]ModelStat       `json:"model_stats"`
	EndpointStats    map[string]EndpointStat    `json:"endpoint_stats"`
	GroupStats       map[string]GroupStat       `json:"group_stats"`
}

// ModelStat æ¨¡å‹ç»Ÿè®¡
type ModelStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// EndpointStat ç«¯ç‚¹ç»Ÿè®¡
type EndpointStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// GroupStat ç»„ç»Ÿè®¡
type GroupStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// RequestEvent è¡¨ç¤ºè¯·æ±‚äº‹ä»¶
type RequestEvent struct {
	Type      string      `json:"type"`      // "start", "flexible_update", "success", "final_failure", "complete", "failed_request_tokens", "token_recovery"
	RequestID string      `json:"request_id"`
	Timestamp time.Time   `json:"timestamp"`
	Data      interface{} `json:"data"` // æ ¹æ®Typeä¸åŒè€Œå˜åŒ–
}

// RequestStartData è¯·æ±‚å¼€å§‹äº‹ä»¶æ•°æ®
type RequestStartData struct {
	ClientIP    string `json:"client_ip"`
	UserAgent   string `json:"user_agent"`
	Method      string `json:"method"`
	Path        string `json:"path"`
	IsStreaming bool   `json:"is_streaming"` // æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
}

// RequestUpdateData è¯·æ±‚æ›´æ–°äº‹ä»¶æ•°æ®
type RequestUpdateData struct {
	EndpointName  string `json:"endpoint_name"`
	GroupName     string `json:"group_name"`
	Status        string `json:"status"`
	RetryCount    int    `json:"retry_count"`
	HTTPStatus    int    `json:"http_status"`
	// ğŸš€ [çŠ¶æ€æœºé‡æ„] Phase 2: æ–°å¢å¤±è´¥åŸå› å’Œå–æ¶ˆåŸå› å­—æ®µ
	FailureReason string `json:"failure_reason,omitempty"`
	CancelReason  string `json:"cancel_reason,omitempty"`
}

// RequestCompleteData è¯·æ±‚å®Œæˆäº‹ä»¶æ•°æ®
type RequestCompleteData struct {
	ModelName           string        `json:"model_name"`
	InputTokens         int64         `json:"input_tokens"`
	OutputTokens        int64         `json:"output_tokens"`
	CacheCreationTokens int64         `json:"cache_creation_tokens"`
	CacheReadTokens     int64         `json:"cache_read_tokens"`
	Duration            time.Duration `json:"duration"`
	FailureReason       string        `json:"failure_reason,omitempty"` // å¯é€‰ï¼šå¤±è´¥åŸå› 
}

// TokenUsage tokenä½¿ç”¨ç»Ÿè®¡
type TokenUsage struct {
	InputTokens         int64
	OutputTokens        int64
	CacheCreationTokens int64
	CacheReadTokens     int64
}

// ModelPricing æ¨¡å‹å®šä»·é…ç½®
type ModelPricing struct {
	Input         float64 `yaml:"input"`          // per 1M tokens
	Output        float64 `yaml:"output"`         // per 1M tokens
	CacheCreation float64 `yaml:"cache_creation"` // per 1M tokens (ç¼“å­˜åˆ›å»º)
	CacheRead     float64 `yaml:"cache_read"`     // per 1M tokens (ç¼“å­˜è¯»å–)
}

// Config ä½¿ç”¨è·Ÿè¸ªé…ç½®
type Config struct {
	Enabled         bool                     `yaml:"enabled"`

	// å‘åå…¼å®¹ï¼šä¿ç•™åŸæœ‰çš„ database_path é…ç½®
	DatabasePath    string                   `yaml:"database_path"`

	// æ–°å¢ï¼šæ•°æ®åº“é…ç½®ï¼ˆä¼˜å…ˆçº§é«˜äº DatabasePathï¼‰
	Database        *config.DatabaseBackendConfig  `yaml:"database,omitempty"`

	BufferSize      int                      `yaml:"buffer_size"`
	BatchSize       int                      `yaml:"batch_size"`
	FlushInterval   time.Duration            `yaml:"flush_interval"`
	MaxRetry        int                      `yaml:"max_retry"`
	RetentionDays   int                      `yaml:"retention_days"`
	CleanupInterval time.Duration            `yaml:"cleanup_interval"`
	ModelPricing    map[string]ModelPricing  `yaml:"model_pricing"`
	DefaultPricing  ModelPricing             `yaml:"default_pricing"`
}

// WriteRequest å†™æ“ä½œè¯·æ±‚
type WriteRequest struct {
	Query     string
	Args      []interface{}
	Response  chan error
	Context   context.Context
	EventType string  // ç”¨äºè°ƒè¯•å’Œç›‘æ§
}

// UpdateOptions ç»Ÿä¸€çš„è¯·æ±‚æ›´æ–°é€‰é¡¹
// æ”¯æŒå¯é€‰å­—æ®µæ›´æ–°ï¼Œåªæ›´æ–°énilçš„å­—æ®µ
type UpdateOptions struct {
	EndpointName  *string        // ç«¯ç‚¹åç§°
	GroupName     *string        // ç»„åç§°
	Status        *string        // çŠ¶æ€
	RetryCount    *int           // é‡è¯•æ¬¡æ•°
	HttpStatus    *int           // HTTPçŠ¶æ€ç 
	ModelName     *string        // æ¨¡å‹åç§°
	EndTime       *time.Time     // ç»“æŸæ—¶é—´
	Duration      *time.Duration // æŒç»­æ—¶é—´
	FailureReason *string        // å¤±è´¥åŸå› ï¼ˆç”¨äºä¸­é—´è¿‡ç¨‹è®°å½•ï¼‰
}

// UsageTracker ä½¿ç”¨è·Ÿè¸ªå™¨
type UsageTracker struct {
	// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
	db           *sql.DB
	eventChan    chan RequestEvent
	config       *Config
	pricing      map[string]ModelPricing
	ctx          context.Context
	cancel       context.CancelFunc
	wg           sync.WaitGroup
	mu           sync.RWMutex
	errorHandler *ErrorHandler

	// æ—¶åŒºæ”¯æŒ
	location     *time.Location  // é…ç½®çš„æ—¶åŒº

	// æ–°å¢ï¼šæ•°æ®åº“é€‚é…å™¨
	adapter    DatabaseAdapter   // æ•°æ®åº“é€‚é…å™¨æ¥å£

	// æ–°å¢ï¼šè¯»å†™åˆ†ç¦»ç»„ä»¶ï¼ˆä»é€‚é…å™¨è·å–ï¼‰
	readDB     *sql.DB           // è¯»è¿æ¥æ±  (å¤šè¿æ¥)
	writeDB    *sql.DB           // å†™è¿æ¥ (å•è¿æ¥)
	writeQueue chan WriteRequest // å†™æ“ä½œé˜Ÿåˆ—
	writeMu    sync.Mutex        // å†™æ“ä½œä¿æŠ¤é”
	writeWg    sync.WaitGroup    // å†™å¤„ç†å™¨ç­‰å¾…ç»„
}

// NewUsageTracker åˆ›å»ºæ–°çš„ä½¿ç”¨è·Ÿè¸ªå™¨
func NewUsageTracker(config *Config, globalTimezone ...string) (*UsageTracker, error) {
	if config == nil || !config.Enabled {
		return &UsageTracker{config: config}, nil
	}

	// è®¾ç½®é»˜è®¤å€¼
	if config.BufferSize <= 0 {
		config.BufferSize = 1000
	}
	if config.BatchSize <= 0 {
		config.BatchSize = 100
	}
	if config.FlushInterval <= 0 {
		config.FlushInterval = 30 * time.Second
	}
	if config.MaxRetry <= 0 {
		config.MaxRetry = 3
	}
	if config.CleanupInterval <= 0 {
		config.CleanupInterval = 24 * time.Hour  // é»˜è®¤24å°æ—¶æ¸…ç†ä¸€æ¬¡
	}

	// æ„å»ºæ•°æ®åº“é…ç½®
	tz := ""
	if len(globalTimezone) > 0 {
		tz = globalTimezone[0]
	}
	dbConfig, err := buildDatabaseConfig(config, tz)
	if err != nil {
		return nil, fmt.Errorf("failed to build database config: %w", err)
	}

	// åˆ›å»ºæ•°æ®åº“é€‚é…å™¨
	adapter, err := NewDatabaseAdapter(dbConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create database adapter: %w", err)
	}

	// æ‰“å¼€æ•°æ®åº“è¿æ¥
	if err := adapter.Open(); err != nil {
		return nil, fmt.Errorf("failed to open database: %w", err)
	}

	// è·å–è¯»å†™è¿æ¥ï¼ˆä»é€‚é…å™¨ï¼‰
	readDB := adapter.GetReadDB()
	writeDB := adapter.GetWriteDB()

	// ä¿æŒåŸæœ‰dbå­—æ®µå…¼å®¹æ€§ï¼ˆæŒ‡å‘readDBï¼‰
	db := readDB

	ctx, cancel := context.WithCancel(context.Background())

	// åˆå§‹åŒ–æ—¶åŒº
	timezone := dbConfig.Timezone
	if timezone == "" {
		timezone = "Asia/Shanghai" // é»˜è®¤æ—¶åŒº
	}
	location, err := time.LoadLocation(timezone)
	if err != nil {
		slog.Warn("åŠ è½½æ—¶åŒºå¤±è´¥ï¼Œä½¿ç”¨Asia/Shanghai", "timezone", timezone, "error", err)
		location, _ = time.LoadLocation("Asia/Shanghai")
		if location == nil {
			location = time.FixedZone("CST", 8*3600) // åå¤‡æ–¹æ¡ˆï¼šå›ºå®š+8æ—¶åŒº
		}
	}

	ut := &UsageTracker{
		// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
		db:        db,        // å…¼å®¹æ€§ï¼šæŒ‡å‘readDB
		eventChan: make(chan RequestEvent, config.BufferSize),
		config:    config,
		pricing:   config.ModelPricing,
		ctx:       ctx,
		cancel:    cancel,

		// æ—¶åŒºæ”¯æŒ
		location:    location,

		// æ–°å¢ï¼šæ•°æ®åº“é€‚é…å™¨
		adapter: adapter,

		// è¯»å†™åˆ†ç¦»ç»„ä»¶ï¼ˆä»é€‚é…å™¨è·å–ï¼‰
		readDB:     readDB,
		writeDB:    writeDB,
		writeQueue: make(chan WriteRequest, config.BufferSize), // ä¸äº‹ä»¶é˜Ÿåˆ—å®¹é‡ä¸€è‡´
	}

	// åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
	ut.errorHandler = NewErrorHandler(ut, slog.Default())

	// åˆå§‹åŒ–æ•°æ®åº“Schemaï¼ˆä½¿ç”¨é€‚é…å™¨ï¼‰
	if err := ut.initDatabaseWithAdapter(); err != nil {
		cancel()
		adapter.Close()
		return nil, fmt.Errorf("failed to initialize database: %w", err)
	}

	// å¯åŠ¨å†™æ“ä½œå¤„ç†å™¨
	go ut.processWriteQueue()

	// å¯åŠ¨å¼‚æ­¥äº‹ä»¶å¤„ç†å™¨
	ut.wg.Add(1)
	go ut.processEvents()

	// å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
	ut.wg.Add(1)
	go ut.periodicCleanup()

	// å¯åŠ¨å®šæœŸå¤‡ä»½ä»»åŠ¡
	ut.wg.Add(1)
	go ut.periodicBackup()

	slog.Info("âœ… ä½¿ç”¨è·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ",
		"database_type", adapter.GetDatabaseType(),
		"buffer_size", config.BufferSize,
		"batch_size", config.BatchSize)

	return ut, nil
}

// now è¿”å›å½“å‰é…ç½®æ—¶åŒºçš„æ—¶é—´
func (ut *UsageTracker) now() time.Time {
	if ut.location == nil {
		return time.Now() // åå¤‡æ–¹æ¡ˆ
	}
	return time.Now().In(ut.location)
}

// buildDatabaseConfig ä»Configæ„å»ºDatabaseConfig
func buildDatabaseConfig(config *Config, globalTimezone string) (DatabaseConfig, error) {
	var dbConfig DatabaseConfig

	// ä¼˜å…ˆä½¿ç”¨æ–°çš„Databaseé…ç½®
	if config.Database != nil {
		dbConfig.Type = config.Database.Type
		dbConfig.DatabasePath = config.Database.Path  // ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
		dbConfig.Host = config.Database.Host
		dbConfig.Port = config.Database.Port
		dbConfig.Database = config.Database.Database
		dbConfig.Username = config.Database.Username
		dbConfig.Password = config.Database.Password
		dbConfig.MaxOpenConns = config.Database.MaxOpenConns
		dbConfig.MaxIdleConns = config.Database.MaxIdleConns
		dbConfig.ConnMaxLifetime = config.Database.ConnMaxLifetime
		dbConfig.ConnMaxIdleTime = config.Database.ConnMaxIdleTime
		dbConfig.Charset = config.Database.Charset
		dbConfig.Timezone = config.Database.Timezone
	} else {
		// å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰çš„DatabasePathé…ç½®
		dbConfig.Type = "sqlite" // é»˜è®¤ä¸ºSQLite
		dbConfig.DatabasePath = config.DatabasePath
		if dbConfig.DatabasePath == "" {
			dbConfig.DatabasePath = "data/usage.db"
		}
	}

	// æ—¶åŒºçº§è”é€»è¾‘ï¼šä¼˜å…ˆçº§ database.timezone > global.timezone > é»˜è®¤å€¼
	if dbConfig.Timezone == "" {
		// æ•°æ®åº“é…ç½®æ²¡æœ‰æŒ‡å®šæ—¶åŒºï¼Œå°è¯•ä½¿ç”¨å…¨å±€æ—¶åŒº
		if globalTimezone != "" {
			dbConfig.Timezone = globalTimezone
		}
		// å¦‚æœå…¨å±€æ—¶åŒºä¹Ÿæ²¡æœ‰ï¼ŒsetDefaultConfigä¼šè®¾ç½®é»˜è®¤å€¼
	}

	return dbConfig, nil
}

// initDatabaseWithAdapter ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–æ•°æ®åº“
func (ut *UsageTracker) initDatabaseWithAdapter() error {
	if ut.adapter == nil {
		return fmt.Errorf("database adapter not initialized")
	}

	// ä½¿ç”¨é€‚é…å™¨åˆå§‹åŒ–Schema
	if err := ut.adapter.InitSchema(); err != nil {
		return fmt.Errorf("failed to initialize database schema: %w", err)
	}

	slog.Info("æ•°æ®åº“Schemaåˆå§‹åŒ–å®Œæˆ",
		"database_type", ut.adapter.GetDatabaseType())

	return nil
}

// Close å…³é—­ä½¿ç”¨è·Ÿè¸ªå™¨
func (ut *UsageTracker) Close() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}

	// å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å…³é—­
	ut.mu.RLock()
	if ut.cancel == nil {
		ut.mu.RUnlock()
		return nil // å·²ç»å…³é—­è¿‡
	}
	ut.mu.RUnlock()

	slog.Info("Shutting down usage tracker...")
	
	// å–æ¶ˆä¸Šä¸‹æ–‡ï¼ˆä¸éœ€è¦æŒæœ‰é”ï¼‰
	ut.cancel()

	// ç­‰å¾…æ‰€æœ‰åç¨‹å®Œæˆï¼ˆåŒ…æ‹¬å†™å¤„ç†å™¨ï¼‰
	ut.wg.Wait()
	ut.writeWg.Wait() // ç­‰å¾…å†™å¤„ç†å™¨å®Œæˆ

	// ç°åœ¨å¯ä»¥å®‰å…¨åœ°æŒæœ‰å†™é”è¿›è¡Œæ¸…ç†
	ut.mu.Lock()
	defer ut.mu.Unlock()
	
	ut.cancel = nil // æ ‡è®°ä¸ºå·²å…³é—­

	// å…³é—­äº‹ä»¶é€šé“
	if ut.eventChan != nil {
		close(ut.eventChan)
		ut.eventChan = nil
	}
	
	// å…³é—­å†™æ“ä½œé˜Ÿåˆ—
	if ut.writeQueue != nil {
		close(ut.writeQueue)
		ut.writeQueue = nil
	}

	// å…³é—­æ•°æ®åº“é€‚é…å™¨ï¼ˆä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰è¿æ¥ï¼‰
	if ut.adapter != nil {
		if err := ut.adapter.Close(); err != nil {
			slog.Error("Failed to close database adapter", "error", err)
			return fmt.Errorf("failed to close database adapter: %w", err)
		}
		ut.adapter = nil
	}

	// æ¸…ç†è¿æ¥å¼•ç”¨ï¼ˆè¿™äº›ç°åœ¨ç”±adapterç®¡ç†ï¼‰
	ut.readDB = nil
	ut.writeDB = nil
	ut.db = nil

	slog.Info("âœ… ä½¿ç”¨è·Ÿè¸ªå™¨å…³é—­å®Œæˆ")
	return nil
}

// RecordRequestStart è®°å½•è¯·æ±‚å¼€å§‹
func (ut *UsageTracker) RecordRequestStart(requestID, clientIP, userAgent, method, path string, isStreaming bool) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	event := RequestEvent{
		Type:      "start",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestStartData{
			ClientIP:    clientIP,
			UserAgent:   userAgent,
			Method:      method,
			Path:        path,
			IsStreaming: isStreaming,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		// ç¼“å†²åŒºæ»¡æ—¶çš„å¤„ç†ç­–ç•¥
		slog.Warn("Usage tracking event buffer full, dropping start event", 
			"request_id", requestID)
	}
}

// RecordRequestUpdate ç»Ÿä¸€çš„è¯·æ±‚æ›´æ–°æ–¹æ³•
// æ”¯æŒå¯é€‰å­—æ®µæ›´æ–°ï¼Œåªæ›´æ–°énilçš„å­—æ®µï¼Œé€‚ç”¨äºæ‰€æœ‰ä¸­é—´è¿‡ç¨‹çŠ¶æ€å˜æ›´
func (ut *UsageTracker) RecordRequestUpdate(requestID string, opts UpdateOptions) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	event := RequestEvent{
		Type:      "flexible_update",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data:      opts,
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping flexible_update event",
			"request_id", requestID)
	}
}

// RecordRequestSuccess è®°å½•è¯·æ±‚æˆåŠŸå®Œæˆ
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰æˆåŠŸç›¸å…³å­—æ®µï¼šstatus='completed', end_time, duration_ms, Tokenå’Œæˆæœ¬ä¿¡æ¯
func (ut *UsageTracker) RecordRequestSuccess(requestID, modelName string, tokens *TokenUsage, duration time.Duration) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// ğŸš€ [æ¶æ„ä¿®å¤] æ”¯æŒ nil tokensï¼Œç¡®ä¿è€—æ—¶ä¿¡æ¯æ€»æ˜¯è¢«è®°å½•
	var inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64
	if tokens != nil {
		inputTokens = tokens.InputTokens
		outputTokens = tokens.OutputTokens
		cacheCreationTokens = tokens.CacheCreationTokens
		cacheReadTokens = tokens.CacheReadTokens
	}
	// å¦‚æœ tokens ä¸º nilï¼Œæ‰€æœ‰ token å­—æ®µéƒ½æ˜¯ 0ï¼Œä½† duration ä»ç„¶ä¼šè¢«è®°å½•

	event := RequestEvent{
		Type:      "success",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         inputTokens,
			OutputTokens:        outputTokens,
			CacheCreationTokens: cacheCreationTokens,
			CacheReadTokens:     cacheReadTokens,
			Duration:            duration,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping success event",
			"request_id", requestID)
	}
}

// RecordRequestFinalFailure è®°å½•è¯·æ±‚æœ€ç»ˆå¤±è´¥æˆ–å–æ¶ˆ
// ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰å¤±è´¥/å–æ¶ˆç›¸å…³å­—æ®µï¼šstatus, end_time, duration_ms, failure_reason/cancel_reason, http_status_code, å¯é€‰Token
func (ut *UsageTracker) RecordRequestFinalFailure(requestID, status, reason, errorDetail string, duration time.Duration, httpStatus int, tokens *TokenUsage) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	// å¤„ç†Tokenä¿¡æ¯ï¼ˆå¤±è´¥/å–æ¶ˆæ—¶å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ï¼‰
	var inputTokens, outputTokens, cacheCreationTokens, cacheReadTokens int64
	if tokens != nil {
		inputTokens = tokens.InputTokens
		outputTokens = tokens.OutputTokens
		cacheCreationTokens = tokens.CacheCreationTokens
		cacheReadTokens = tokens.CacheReadTokens
	}

	event := RequestEvent{
		Type:      "final_failure",
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: map[string]interface{}{
			"status":               status,    // "failed" or "cancelled"
			"reason":               reason,    // failure_reason or cancel_reason
			"error_detail":         errorDetail,
			"duration":             duration,
			"http_status":          httpStatus, // HTTPçŠ¶æ€ç 
			"input_tokens":         inputTokens,
			"output_tokens":        outputTokens,
			"cache_creation_tokens": cacheCreationTokens,
			"cache_read_tokens":    cacheReadTokens,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping final_failure event",
			"request_id", requestID)
	}
}

// RecordFailedRequestTokens è®°å½•å¤±è´¥è¯·æ±‚çš„Tokenä½¿ç”¨
// åªè®°å½•Tokenç»Ÿè®¡ï¼Œä¸å½±å“è¯·æ±‚çŠ¶æ€
func (ut *UsageTracker) RecordFailedRequestTokens(requestID, modelName string, tokens *TokenUsage, duration time.Duration, failureReason string) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	// åˆ›å»ºç‰¹æ®Šçš„å¤±è´¥è¯·æ±‚å®Œæˆäº‹ä»¶
	event := RequestEvent{
		Type:      "failed_request_tokens", // æ–°çš„äº‹ä»¶ç±»å‹
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         tokens.InputTokens,
			OutputTokens:        tokens.OutputTokens,
			CacheCreationTokens: tokens.CacheCreationTokens,
			CacheReadTokens:     tokens.CacheReadTokens,
			Duration:            duration,
			FailureReason:       failureReason, // æ–°å¢å¤±è´¥åŸå› å­—æ®µ
		},
	}

	select {
	case ut.eventChan <- event:
		slog.Debug(fmt.Sprintf("ğŸ’¾ [å¤±è´¥Tokenäº‹ä»¶] [%s] åŸå› : %s, æ¨¡å‹: %s", requestID, failureReason, modelName))
	default:
		slog.Warn("Usage tracking event buffer full, dropping failed request tokens event",
			"request_id", requestID)
	}
}

// RecoverRequestTokens æ¢å¤è¯·æ±‚çš„Tokenä½¿ç”¨ç»Ÿè®¡
// ğŸ”§ [Fallbackä¿®å¤] ä¸“ç”¨äºdebugæ–‡ä»¶æ¢å¤åœºæ™¯ï¼Œä»…æ›´æ–°Tokenå­—æ®µï¼Œä¸è§¦å‘çŠ¶æ€å˜æ›´
func (ut *UsageTracker) RecoverRequestTokens(requestID, modelName string, tokens *TokenUsage) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	// åˆ›å»ºä¸“é—¨çš„Tokenæ¢å¤äº‹ä»¶
	event := RequestEvent{
		Type:      "token_recovery", // ä¸“ç”¨äº‹ä»¶ç±»å‹
		RequestID: requestID,
		Timestamp: ut.now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         tokens.InputTokens,
			OutputTokens:        tokens.OutputTokens,
			CacheCreationTokens: tokens.CacheCreationTokens,
			CacheReadTokens:     tokens.CacheReadTokens,
			// æ³¨æ„ï¼šDurationè®¾ä¸º0ï¼Œä¸æ›´æ–°æ—¶é—´ç›¸å…³å­—æ®µ
			Duration: 0,
		},
	}

	select {
	case ut.eventChan <- event:
		slog.Info(fmt.Sprintf("ğŸ”§ [Tokenæ¢å¤äº‹ä»¶] [%s] æ¨¡å‹: %s, è¾“å…¥: %d, è¾“å‡º: %d",
			requestID, modelName, tokens.InputTokens, tokens.OutputTokens))
	default:
		slog.Warn("Usage tracking event buffer full, dropping token recovery event",
			"request_id", requestID)
	}
}

// UpdatePricing æ›´æ–°æ¨¡å‹å®šä»·ï¼ˆè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°ï¼‰
func (ut *UsageTracker) UpdatePricing(pricing map[string]ModelPricing) {
	ut.mu.Lock()
	defer ut.mu.Unlock()
	
	ut.pricing = pricing
	slog.Info("Model pricing updated", "model_count", len(pricing))
}

// GetDatabaseStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…è£…æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetDatabaseStats(ctx context.Context) (*DatabaseStats, error) {
	if ut.config == nil || !ut.config.Enabled {
		return nil, fmt.Errorf("usage tracking not enabled")
	}
	if ut.db == nil {
		return nil, fmt.Errorf("database not initialized")
	}
	return ut.getDatabaseStatsInternal(ctx)
}

// HealthCheck æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬åŠŸèƒ½ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) HealthCheck(ctx context.Context) error {
	if ut.config == nil || !ut.config.Enabled {
		return nil // å¦‚æœæœªå¯ç”¨ï¼Œè®¤ä¸ºæ˜¯å¥åº·çš„
	}
	
	if ut.readDB == nil {
		return fmt.Errorf("read database not initialized")
	}
	
	// æµ‹è¯•è¯»æ•°æ®åº“è¿æ¥
	if err := ut.readDB.PingContext(ctx); err != nil {
		return fmt.Errorf("read database ping failed: %w", err)
	}
	
	// æµ‹è¯•å†™æ•°æ®åº“è¿æ¥
	if ut.writeDB != nil {
		if err := ut.writeDB.PingContext(ctx); err != nil {
			return fmt.Errorf("write database ping failed: %w", err)
		}
	}
	
	// æµ‹è¯•åŸºæœ¬æŸ¥è¯¢ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	var count int
	err := ut.readDB.QueryRowContext(ctx, "SELECT COUNT(*) FROM sqlite_master WHERE type='table'").Scan(&count)
	if err != nil {
		return fmt.Errorf("database query test failed: %w", err)
	}
	
	// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
	if count < 2 { // è‡³å°‘åº”è¯¥æœ‰ request_logs å’Œ usage_summary ä¸¤ä¸ªè¡¨
		return fmt.Errorf("database schema incomplete: expected at least 2 tables, found %d", count)
	}
	
	// æ£€æŸ¥äº‹ä»¶å¤„ç†é€šé“æ˜¯å¦æ­£å¸¸
	select {
	case <-ut.ctx.Done():
		return fmt.Errorf("usage tracker context cancelled")
	default:
		// ä¸Šä¸‹æ–‡æ­£å¸¸
	}
	
	// æ£€æŸ¥äº‹ä»¶é€šé“å®¹é‡
	if ut.eventChan != nil {
		channelLoad := float64(len(ut.eventChan)) / float64(cap(ut.eventChan)) * 100
		if channelLoad > 90 {
			return fmt.Errorf("event channel overloaded: %.1f%% capacity used", channelLoad)
		}
	}
	
	// æ£€æŸ¥å†™é˜Ÿåˆ—å®¹é‡
	if ut.writeQueue != nil {
		writeQueueLoad := float64(len(ut.writeQueue)) / float64(cap(ut.writeQueue)) * 100
		if writeQueueLoad > 90 {
			return fmt.Errorf("write queue overloaded: %.1f%% capacity used", writeQueueLoad)
		}
	}
	
	return nil
}

// ForceFlush å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰å¾…å¤„ç†äº‹ä»¶
func (ut *UsageTracker) ForceFlush() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}
	
	// å°è¯•å‘é€ä¸€ä¸ªç‰¹æ®Šäº‹ä»¶æ¥è§¦å‘æ‰¹å¤„ç†
	flushEvent := RequestEvent{
		Type:      "flush",
		RequestID: "force-flush-" + ut.now().Format("20060102150405"),
		Timestamp: ut.now(),
		Data:      nil,
	}
	
	select {
	case ut.eventChan <- flushEvent:
		slog.Info("Force flush event sent")
		return nil
	default:
		return fmt.Errorf("event channel full, cannot force flush")
	}
}

// GetPricing è·å–æ¨¡å‹å®šä»·
func (ut *UsageTracker) GetPricing(modelName string) ModelPricing {
	ut.mu.RLock()
	defer ut.mu.RUnlock()
	
	if pricing, exists := ut.pricing[modelName]; exists {
		return pricing
	}
	return ut.config.DefaultPricing
}

// GetConfiguredModels è·å–é…ç½®ä¸­çš„æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
func (ut *UsageTracker) GetConfiguredModels() []string {
	ut.mu.RLock()
	defer ut.mu.RUnlock()
	
	models := make([]string, 0, len(ut.pricing))
	for modelName := range ut.pricing {
		models = append(models, modelName)
	}
	
	return models
}

// GetUsageSummary è·å–ä½¿ç”¨æ‘˜è¦ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetUsageSummary(ctx context.Context, startTime, endTime time.Time) ([]UsageSummary, error) {
	opts := &QueryOptions{
		StartDate: &startTime,
		EndDate:   &endTime,
		Limit:     100,
	}
	return ut.QueryUsageSummary(ctx, opts)
}

// GetRequestLogs è·å–è¯·æ±‚æ—¥å¿—ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetRequestLogs(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string, limit, offset int) ([]RequestDetail, error) {
	opts := &QueryOptions{
		StartDate:    &startTime,
		EndDate:      &endTime,
		ModelName:    modelName,
		EndpointName: endpointName,
		GroupName:    groupName,
		Limit:        limit,
		Offset:       offset,
	}
	return ut.QueryRequestDetails(ctx, opts)
}

// GetUsageStats è·å–ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼Œä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) GetUsageStats(ctx context.Context, startTime, endTime time.Time) (*UsageStatsDetailed, error) {
	if ut.readDB == nil {
		return nil, fmt.Errorf("read database not initialized")
	}

	query := `SELECT 
		COUNT(*) as total_requests,
		SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as success_requests,
		SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as error_requests,
		SUM(input_tokens + output_tokens + cache_creation_tokens + cache_read_tokens) as total_tokens,
		SUM(total_cost_usd) as total_cost
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ?`
	
	var stats UsageStatsDetailed
	err := ut.readDB.QueryRowContext(ctx, query, startTime, endTime).Scan(
		&stats.TotalRequests,
		&stats.SuccessRequests,
		&stats.ErrorRequests,
		&stats.TotalTokens,
		&stats.TotalCost,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to query detailed usage stats: %w", err)
	}
	
	// è·å–æ¨¡å‹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	modelQuery := `SELECT model_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND model_name IS NOT NULL AND model_name != ''
		GROUP BY model_name`
	
	rows, err := ut.readDB.QueryContext(ctx, modelQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query model stats: %w", err)
	}
	defer rows.Close()
	
	stats.ModelStats = make(map[string]ModelStat)
	for rows.Next() {
		var modelName string
		var requests int64
		var cost float64
		if err := rows.Scan(&modelName, &requests, &cost); err != nil {
			continue
		}
		stats.ModelStats[modelName] = ModelStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	// è·å–ç«¯ç‚¹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	endpointQuery := `SELECT endpoint_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND endpoint_name IS NOT NULL AND endpoint_name != ''
		GROUP BY endpoint_name`
	
	rows2, err := ut.readDB.QueryContext(ctx, endpointQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query endpoint stats: %w", err)
	}
	defer rows2.Close()
	
	stats.EndpointStats = make(map[string]EndpointStat)
	for rows2.Next() {
		var endpointName string
		var requests int64
		var cost float64
		if err := rows2.Scan(&endpointName, &requests, &cost); err != nil {
			continue
		}
		stats.EndpointStats[endpointName] = EndpointStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	// è·å–ç»„ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	groupQuery := `SELECT group_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND group_name IS NOT NULL AND group_name != ''
		GROUP BY group_name`
	
	rows3, err := ut.readDB.QueryContext(ctx, groupQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query group stats: %w", err)
	}
	defer rows3.Close()
	
	stats.GroupStats = make(map[string]GroupStat)
	for rows3.Next() {
		var groupName string
		var requests int64
		var cost float64
		if err := rows3.Scan(&groupName, &requests, &cost); err != nil {
			continue
		}
		stats.GroupStats[groupName] = GroupStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	return &stats, nil
}

// ExportToCSV å¯¼å‡ºä¸ºCSVæ ¼å¼
func (ut *UsageTracker) ExportToCSV(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for CSV export: %w", err)
	}
	
	// CSV header
	csv := "request_id,client_ip,user_agent,method,path,start_time,end_time,duration_ms,endpoint_name,group_name,model_name,status,http_status_code,retry_count,input_tokens,output_tokens,cache_creation_tokens,cache_read_tokens,input_cost_usd,output_cost_usd,cache_creation_cost_usd,cache_read_cost_usd,total_cost_usd,created_at,updated_at\n"
	
	// CSV rows
	for _, log := range logs {
		endTime := ""
		if log.EndTime != nil {
			endTime = log.EndTime.Format(time.RFC3339)
		}
		
		durationMs := ""
		if log.DurationMs != nil {
			durationMs = fmt.Sprintf("%d", *log.DurationMs)
		}
		
		httpStatus := ""
		if log.HTTPStatusCode != nil {
			httpStatus = fmt.Sprintf("%d", *log.HTTPStatusCode)
		}
		
		csv += fmt.Sprintf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%d,%d,%d,%d,%d,%.6f,%.6f,%.6f,%.6f,%.6f,%s,%s\n",
			log.RequestID, log.ClientIP, log.UserAgent, log.Method, log.Path,
			log.StartTime.Format(time.RFC3339), endTime, durationMs,
			log.EndpointName, log.GroupName, log.ModelName, log.Status,
			httpStatus, log.RetryCount,
			log.InputTokens, log.OutputTokens, log.CacheCreationTokens, log.CacheReadTokens,
			log.InputCostUSD, log.OutputCostUSD, log.CacheCreationCostUSD, log.CacheReadCostUSD, log.TotalCostUSD,
			log.CreatedAt.Format(time.RFC3339), log.UpdatedAt.Format(time.RFC3339),
		)
	}
	
	return []byte(csv), nil
}

// ExportToJSON å¯¼å‡ºä¸ºJSONæ ¼å¼
func (ut *UsageTracker) ExportToJSON(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for JSON export: %w", err)
	}
	
	// ä½¿ç”¨æ ‡å‡†åº“çš„jsonåŒ…åºåˆ—åŒ–
	jsonBytes, err := json.Marshal(logs)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal logs to JSON: %w", err)
	}
	
	return jsonBytes, nil
}

// processWriteQueue å¯åŠ¨å†™æ“ä½œé˜Ÿåˆ—å¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼Œç¡®ä¿ç¨³å®šæ€§ï¼‰
func (ut *UsageTracker) processWriteQueue() {
	ut.writeWg.Add(1)
	defer ut.writeWg.Done()
	slog.Debug("Write processor started")

	for {
		select {
		case writeReq := <-ut.writeQueue:
			err := ut.executeWriteSimple(writeReq)
			writeReq.Response <- err

		case <-ut.ctx.Done():
			slog.Debug("Write processor stopped")
			return
		}
	}
}

// executeWriteSimple æ‰§è¡Œç®€å•å†™æ“ä½œï¼ˆé¿å…å¤æ‚çš„æ‰¹å¤„ç†ï¼‰
func (ut *UsageTracker) executeWriteSimple(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()
	
	ctx, cancel := context.WithTimeout(req.Context, 30*time.Second)
	defer cancel()
	
	// ç›´æ¥æ‰§è¡Œï¼Œä¸ä½¿ç”¨äº‹åŠ¡ï¼ˆå¯¹äºç®€å•INSERT/UPDATEï¼Œä¸ä¸€å®šéœ€è¦äº‹åŠ¡ï¼‰
	if req.EventType == "vacuum" {
		// VACUUMä¸èƒ½åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œ
		_, err := ut.writeDB.ExecContext(ctx, req.Query, req.Args...)
		return err
	}
	
	// å…¶ä»–æ“ä½œä½¿ç”¨çŸ­äº‹åŠ¡
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	
	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Debug("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()
	
	_, err = tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}
	
	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}
	
	committed = true
	return nil
}

// executeWrite æ‰§è¡Œå•ä¸ªå†™æ“ä½œ
func (ut *UsageTracker) executeWrite(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()
	
	ctx, cancel := context.WithTimeout(req.Context, 60*time.Second)
	defer cancel()
	
	// å®‰å…¨çš„äº‹åŠ¡å¤„ç†
	return ut.executeWriteTransaction(ctx, req)
}

// executeWriteTransaction æ‰§è¡Œå†™äº‹åŠ¡ï¼ˆä¿®å¤defer tx.Rollback()é—®é¢˜ï¼‰
func (ut *UsageTracker) executeWriteTransaction(ctx context.Context, req WriteRequest) error {
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	
	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Error("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()
	
	_, err = tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}
	
	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}
	
	committed = true  // æ ‡è®°å·²æäº¤ï¼Œé¿å…é‡å¤Rollback
	return nil
}

// initDatabaseWithWriteDB ä½¿ç”¨å†™è¿æ¥åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåŒæ­¥ç­‰å¾…å®Œæˆï¼‰
func (ut *UsageTracker) initDatabaseWithWriteDB() error {
	// è¯»å–å¹¶æ‰§è¡Œ schema SQL
	schemaSQL, err := schemaFS.ReadFile("schema.sql")
	if err != nil {
		return fmt.Errorf("failed to read schema.sql: %w", err)
	}

	// ä½¿ç”¨å†™è¿æ¥ç›´æ¥æ‰§è¡Œ schemaï¼ˆåŒæ­¥æ–¹å¼ï¼Œç¡®ä¿è¡¨åˆ›å»ºå®Œæˆï¼‰
	if _, err := ut.writeDB.Exec(string(schemaSQL)); err != nil {
		return fmt.Errorf("failed to execute schema: %w", err)
	}

	slog.Debug("Database schema initialized successfully with write connection")
	return nil
}