package tracking

import (
	"context"
	"database/sql"
	"embed"
	"encoding/json"
	"fmt"
	"log/slog"
	"os"
	"path/filepath"
	"sync"
	"time"

	_ "modernc.org/sqlite"
)

//go:embed schema.sql
var schemaFS embed.FS

// UsageStatsDetailed è¯¦ç»†çš„ä½¿ç”¨ç»Ÿè®¡
type UsageStatsDetailed struct {
	TotalRequests    int64                      `json:"total_requests"`
	SuccessRequests  int64                      `json:"success_requests"`
	ErrorRequests    int64                      `json:"error_requests"`
	TotalTokens      int64                      `json:"total_tokens"`
	TotalCost        float64                    `json:"total_cost"`
	ModelStats       map[string]ModelStat       `json:"model_stats"`
	EndpointStats    map[string]EndpointStat    `json:"endpoint_stats"`
	GroupStats       map[string]GroupStat       `json:"group_stats"`
}

// ModelStat æ¨¡å‹ç»Ÿè®¡
type ModelStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// EndpointStat ç«¯ç‚¹ç»Ÿè®¡
type EndpointStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// GroupStat ç»„ç»Ÿè®¡
type GroupStat struct {
	RequestCount int64   `json:"request_count"`
	TotalCost    float64 `json:"total_cost"`
}

// RequestEvent è¡¨ç¤ºè¯·æ±‚äº‹ä»¶
type RequestEvent struct {
	Type      string      `json:"type"`      // "start", "update", "update_with_model", "complete", "failed_request_tokens"
	RequestID string      `json:"request_id"`
	Timestamp time.Time   `json:"timestamp"`
	Data      interface{} `json:"data"` // æ ¹æ®Typeä¸åŒè€Œå˜åŒ–
}

// RequestStartData è¯·æ±‚å¼€å§‹äº‹ä»¶æ•°æ®
type RequestStartData struct {
	ClientIP    string `json:"client_ip"`
	UserAgent   string `json:"user_agent"`
	Method      string `json:"method"`
	Path        string `json:"path"`
	IsStreaming bool   `json:"is_streaming"` // æ˜¯å¦ä¸ºæµå¼è¯·æ±‚
}

// RequestUpdateData è¯·æ±‚æ›´æ–°äº‹ä»¶æ•°æ®
type RequestUpdateData struct {
	EndpointName string `json:"endpoint_name"`
	GroupName    string `json:"group_name"`
	Status       string `json:"status"`
	RetryCount   int    `json:"retry_count"`
	HTTPStatus   int    `json:"http_status"`
}

// RequestUpdateDataWithModel åŒ…å«æ¨¡å‹ä¿¡æ¯çš„çŠ¶æ€æ›´æ–°æ•°æ®
type RequestUpdateDataWithModel struct {
	EndpointName string `json:"endpoint_name"`
	GroupName    string `json:"group_name"`
	Status       string `json:"status"`
	RetryCount   int    `json:"retry_count"`
	HTTPStatus   int    `json:"http_status"`
	ModelName    string `json:"model_name"` // æ–°å¢ï¼šæ¨¡å‹ä¿¡æ¯
}

// RequestCompleteData è¯·æ±‚å®Œæˆäº‹ä»¶æ•°æ®
type RequestCompleteData struct {
	ModelName           string        `json:"model_name"`
	InputTokens         int64         `json:"input_tokens"`
	OutputTokens        int64         `json:"output_tokens"`
	CacheCreationTokens int64         `json:"cache_creation_tokens"`
	CacheReadTokens     int64         `json:"cache_read_tokens"`
	Duration            time.Duration `json:"duration"`
	FailureReason       string        `json:"failure_reason,omitempty"` // å¯é€‰ï¼šå¤±è´¥åŸå› 
}

// TokenUsage tokenä½¿ç”¨ç»Ÿè®¡
type TokenUsage struct {
	InputTokens         int64
	OutputTokens        int64
	CacheCreationTokens int64
	CacheReadTokens     int64
}

// ModelPricing æ¨¡å‹å®šä»·é…ç½®
type ModelPricing struct {
	Input         float64 `yaml:"input"`          // per 1M tokens
	Output        float64 `yaml:"output"`         // per 1M tokens
	CacheCreation float64 `yaml:"cache_creation"` // per 1M tokens (ç¼“å­˜åˆ›å»º)
	CacheRead     float64 `yaml:"cache_read"`     // per 1M tokens (ç¼“å­˜è¯»å–)
}

// Config ä½¿ç”¨è·Ÿè¸ªé…ç½®
type Config struct {
	Enabled         bool                     `yaml:"enabled"`
	DatabasePath    string                   `yaml:"database_path"`
	BufferSize      int                      `yaml:"buffer_size"`
	BatchSize       int                      `yaml:"batch_size"`
	FlushInterval   time.Duration            `yaml:"flush_interval"`
	MaxRetry        int                      `yaml:"max_retry"`
	RetentionDays   int                      `yaml:"retention_days"`
	CleanupInterval time.Duration            `yaml:"cleanup_interval"`
	ModelPricing    map[string]ModelPricing  `yaml:"model_pricing"`
	DefaultPricing  ModelPricing             `yaml:"default_pricing"`
}

// WriteRequest å†™æ“ä½œè¯·æ±‚
type WriteRequest struct {
	Query     string
	Args      []interface{}
	Response  chan error
	Context   context.Context
	EventType string  // ç”¨äºè°ƒè¯•å’Œç›‘æ§
}

// UsageTracker ä½¿ç”¨è·Ÿè¸ªå™¨
type UsageTracker struct {
	// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
	db           *sql.DB
	eventChan    chan RequestEvent
	config       *Config
	pricing      map[string]ModelPricing
	ctx          context.Context
	cancel       context.CancelFunc
	wg           sync.WaitGroup
	mu           sync.RWMutex
	errorHandler *ErrorHandler
	
	// æ–°å¢ï¼šè¯»å†™åˆ†ç¦»ç»„ä»¶
	readDB     *sql.DB           // è¯»è¿æ¥æ±  (å¤šè¿æ¥)
	writeDB    *sql.DB           // å†™è¿æ¥ (å•è¿æ¥)
	writeQueue chan WriteRequest // å†™æ“ä½œé˜Ÿåˆ—
	writeMu    sync.Mutex        // å†™æ“ä½œä¿æŠ¤é”
	writeWg    sync.WaitGroup    // å†™å¤„ç†å™¨ç­‰å¾…ç»„
}

// NewUsageTracker åˆ›å»ºæ–°çš„ä½¿ç”¨è·Ÿè¸ªå™¨
func NewUsageTracker(config *Config) (*UsageTracker, error) {
	if config == nil || !config.Enabled {
		return &UsageTracker{config: config}, nil
	}

	// è®¾ç½®é»˜è®¤å€¼
	if config.BufferSize <= 0 {
		config.BufferSize = 1000
	}
	if config.BatchSize <= 0 {
		config.BatchSize = 100
	}
	if config.FlushInterval <= 0 {
		config.FlushInterval = 30 * time.Second
	}
	if config.MaxRetry <= 0 {
		config.MaxRetry = 3
	}

	// ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
	if config.DatabasePath != ":memory:" && config.DatabasePath != "" {
		dbDir := filepath.Dir(config.DatabasePath)
		if err := os.MkdirAll(dbDir, 0755); err != nil {
			return nil, fmt.Errorf("failed to create database directory: %w", err)
		}
	}

	// é’ˆå¯¹:memory:æ•°æ®åº“çš„ç‰¹æ®Šå¤„ç†
	var readDB, writeDB *sql.DB
	var err error
	
	if config.DatabasePath == ":memory:" {
		// å†…å­˜æ•°æ®åº“ï¼šä½¿ç”¨åŒä¸€ä¸ªè¿æ¥ï¼ˆä½†é…ç½®ä¸ºè¯»å†™åˆ†ç¦»æ¨¡å¼ï¼‰
		db, err := sql.Open("sqlite", config.DatabasePath+"?_journal_mode=WAL&_synchronous=NORMAL&_cache_size=10000&_foreign_keys=1&_busy_timeout=60000")
		if err != nil {
			return nil, fmt.Errorf("failed to open memory database: %w", err)
		}
		
		// ä½¿ç”¨åŒä¸€ä¸ªè¿æ¥ï¼Œä½†é€»è¾‘ä¸Šåˆ†ç¦»è¯»å†™
		readDB = db
		writeDB = db
		
		// é…ç½®è¿æ¥æ± å‚æ•°
		db.SetMaxOpenConns(8)    // å†…å­˜æ•°æ®åº“å¯ä»¥æ”¯æŒæ›´å¤šè¿æ¥
		db.SetMaxIdleConns(4)
		db.SetConnMaxLifetime(2 * time.Hour)
	} else {
		// æ–‡ä»¶æ•°æ®åº“ï¼šçœŸæ­£çš„è¯»å†™åˆ†ç¦»
		// æ‰“å¼€è¯»æ•°æ®åº“è¿æ¥ - è¯»æ€§èƒ½ä¼˜åŒ–é…ç½® (æ”¯æŒå¤šå¹¶å‘æŸ¥è¯¢)
		readDB, err = sql.Open("sqlite", config.DatabasePath+"?_journal_mode=WAL&_synchronous=NORMAL&_cache_size=10000&_foreign_keys=1&_busy_timeout=60000")
		if err != nil {
			return nil, fmt.Errorf("failed to open read database: %w", err)
		}

		// æ‰“å¼€å†™æ•°æ®åº“è¿æ¥ - å†™ç¨³å®šæ€§ä¼˜åŒ–é…ç½® (å•è¿æ¥é¿å…é”ç«äº‰)
		writeDB, err = sql.Open("sqlite", config.DatabasePath+"?_journal_mode=WAL&_synchronous=NORMAL&_cache_size=10000&_foreign_keys=1&_busy_timeout=60000")
		if err != nil {
			readDB.Close()
			return nil, fmt.Errorf("failed to open write database: %w", err)
		}

		// é…ç½®è¯»è¿æ¥æ± å‚æ•° - æ”¯æŒé«˜å¹¶å‘æŸ¥è¯¢
		readDB.SetMaxOpenConns(8)    // 8ä¸ªè¯»è¿æ¥ï¼Œæ”¯æŒé«˜å¹¶å‘æŸ¥è¯¢
		readDB.SetMaxIdleConns(4)    // ä¿æŒ4ä¸ªç©ºé—²è¿æ¥
		readDB.SetConnMaxLifetime(2 * time.Hour)

		// é…ç½®å†™è¿æ¥å‚æ•° - å•è¿æ¥é¿å…é”ç«äº‰
		writeDB.SetMaxOpenConns(1)   // å…³é”®ï¼šåªæœ‰1ä¸ªå†™è¿æ¥
		writeDB.SetMaxIdleConns(1)   // ä¿æŒè¿æ¥æ´»è·ƒ
		writeDB.SetConnMaxLifetime(4 * time.Hour)
	}

	// ä¿æŒåŸæœ‰dbå­—æ®µå…¼å®¹æ€§ï¼ˆç”¨äºå‘åå…¼å®¹å’Œåˆå§‹åŒ–ï¼‰
	db := readDB

	ctx, cancel := context.WithCancel(context.Background())

	ut := &UsageTracker{
		// åŸæœ‰å­—æ®µï¼ˆå…¼å®¹æ€§ï¼‰
		db:        db,        // å…¼å®¹æ€§ï¼šæŒ‡å‘readDB
		eventChan: make(chan RequestEvent, config.BufferSize),
		config:    config,
		pricing:   config.ModelPricing,
		ctx:       ctx,
		cancel:    cancel,
		
		// æ–°å¢ï¼šè¯»å†™åˆ†ç¦»ç»„ä»¶
		readDB:     readDB,
		writeDB:    writeDB,
		writeQueue: make(chan WriteRequest, config.BufferSize), // ä¸äº‹ä»¶é˜Ÿåˆ—å®¹é‡ä¸€è‡´
	}

	// åˆå§‹åŒ–é”™è¯¯å¤„ç†å™¨
	ut.errorHandler = NewErrorHandler(ut, slog.Default())

	// åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä½¿ç”¨å†™è¿æ¥ä»¥ç¡®ä¿è¡¨åˆ›å»ºï¼‰
	if err := ut.initDatabaseWithWriteDB(); err != nil {
		cancel()
		readDB.Close()
		writeDB.Close()
		return nil, fmt.Errorf("failed to initialize database: %w", err)
	}

	// å¯åŠ¨å†™æ“ä½œå¤„ç†å™¨
	ut.startWriteProcessor()

	// å¯åŠ¨äº‹ä»¶å¤„ç†åç¨‹
	ut.wg.Add(1)
	go ut.processEvents()

	// å¯åŠ¨å®šæœŸæ¸…ç†åç¨‹
	if config.RetentionDays > 0 && config.CleanupInterval > 0 {
		ut.wg.Add(1)
		go ut.periodicCleanup()
	}

	// å¯åŠ¨å®šæœŸå¤‡ä»½åç¨‹ï¼ˆæ¯6å°æ—¶å¤‡ä»½ä¸€æ¬¡ï¼‰
	ut.wg.Add(1)
	go ut.periodicBackup()

	slog.Info("Usage tracker initialized", 
		"database_path", config.DatabasePath,
		"buffer_size", config.BufferSize,
		"retention_days", config.RetentionDays)

	return ut, nil
}

// Close å…³é—­ä½¿ç”¨è·Ÿè¸ªå™¨
func (ut *UsageTracker) Close() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}

	// å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»å…³é—­
	ut.mu.RLock()
	if ut.cancel == nil {
		ut.mu.RUnlock()
		return nil // å·²ç»å…³é—­è¿‡
	}
	ut.mu.RUnlock()

	slog.Info("Shutting down usage tracker...")
	
	// å–æ¶ˆä¸Šä¸‹æ–‡ï¼ˆä¸éœ€è¦æŒæœ‰é”ï¼‰
	ut.cancel()

	// ç­‰å¾…æ‰€æœ‰åç¨‹å®Œæˆï¼ˆåŒ…æ‹¬å†™å¤„ç†å™¨ï¼‰
	ut.wg.Wait()
	ut.writeWg.Wait() // ç­‰å¾…å†™å¤„ç†å™¨å®Œæˆ

	// ç°åœ¨å¯ä»¥å®‰å…¨åœ°æŒæœ‰å†™é”è¿›è¡Œæ¸…ç†
	ut.mu.Lock()
	defer ut.mu.Unlock()
	
	ut.cancel = nil // æ ‡è®°ä¸ºå·²å…³é—­

	// å…³é—­äº‹ä»¶é€šé“
	if ut.eventChan != nil {
		close(ut.eventChan)
		ut.eventChan = nil
	}
	
	// å…³é—­å†™æ“ä½œé˜Ÿåˆ—
	if ut.writeQueue != nil {
		close(ut.writeQueue)
		ut.writeQueue = nil
	}

	// å…³é—­æ•°æ®åº“è¿æ¥
	var errors []error
	if ut.readDB != nil {
		if err := ut.readDB.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close read database: %w", err))
		}
		ut.readDB = nil
	}
	
	if ut.writeDB != nil {
		if err := ut.writeDB.Close(); err != nil {
			errors = append(errors, fmt.Errorf("failed to close write database: %w", err))
		}
		ut.writeDB = nil
	}
	
	// å…³é—­åŸæœ‰æ•°æ®åº“è¿æ¥ï¼ˆå…¼å®¹æ€§ï¼‰
	if ut.db != nil {
		// ç”±äºdbæŒ‡å‘readDBï¼Œé¿å…é‡å¤å…³é—­
		ut.db = nil
	}
	
	// è¿”å›ç¬¬ä¸€ä¸ªé”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
	if len(errors) > 0 {
		return errors[0]
	}

	return nil
}

// RecordRequestStart è®°å½•è¯·æ±‚å¼€å§‹
func (ut *UsageTracker) RecordRequestStart(requestID, clientIP, userAgent, method, path string, isStreaming bool) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	event := RequestEvent{
		Type:      "start",
		RequestID: requestID,
		Timestamp: time.Now(),
		Data: RequestStartData{
			ClientIP:    clientIP,
			UserAgent:   userAgent,
			Method:      method,
			Path:        path,
			IsStreaming: isStreaming,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		// ç¼“å†²åŒºæ»¡æ—¶çš„å¤„ç†ç­–ç•¥
		slog.Warn("Usage tracking event buffer full, dropping start event", 
			"request_id", requestID)
	}
}

// RecordRequestUpdate è®°å½•è¯·æ±‚çŠ¶æ€æ›´æ–°
func (ut *UsageTracker) RecordRequestUpdate(requestID, endpoint, group, status string, retryCount, httpStatus int) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	event := RequestEvent{
		Type:      "update",
		RequestID: requestID,
		Timestamp: time.Now(),
		Data: RequestUpdateData{
			EndpointName: endpoint,
			GroupName:    group,
			Status:       status,
			RetryCount:   retryCount,
			HTTPStatus:   httpStatus,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping update event", 
			"request_id", requestID)
	}
}

// RecordRequestUpdateWithModel è®°å½•åŒ…å«æ¨¡å‹ä¿¡æ¯çš„çŠ¶æ€æ›´æ–°
func (ut *UsageTracker) RecordRequestUpdateWithModel(requestID, endpoint, group, status string, retryCount, httpStatus int, modelName string) {
	if ut.config == nil || !ut.config.Enabled {
		return
	}

	event := RequestEvent{
		Type:      "update_with_model",
		RequestID: requestID,
		Timestamp: time.Now(),
		Data: RequestUpdateDataWithModel{
			EndpointName: endpoint,
			GroupName:    group,
			Status:       status,
			RetryCount:   retryCount,
			HTTPStatus:   httpStatus,
			ModelName:    modelName, // åŒ…å«æ¨¡å‹ä¿¡æ¯
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping update_with_model event", 
			"request_id", requestID)
	}
}

// RecordRequestComplete è®°å½•è¯·æ±‚å®Œæˆ
func (ut *UsageTracker) RecordRequestComplete(requestID, modelName string, tokens *TokenUsage, duration time.Duration) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	event := RequestEvent{
		Type:      "complete",
		RequestID: requestID,
		Timestamp: time.Now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         tokens.InputTokens,
			OutputTokens:        tokens.OutputTokens,
			CacheCreationTokens: tokens.CacheCreationTokens,
			CacheReadTokens:     tokens.CacheReadTokens,
			Duration:            duration,
		},
	}

	select {
	case ut.eventChan <- event:
		// æˆåŠŸå‘é€äº‹ä»¶
	default:
		slog.Warn("Usage tracking event buffer full, dropping complete event",
			"request_id", requestID)
	}
}

// RecordFailedRequestTokens è®°å½•å¤±è´¥è¯·æ±‚çš„Tokenä½¿ç”¨
// åªè®°å½•Tokenç»Ÿè®¡ï¼Œä¸å½±å“è¯·æ±‚çŠ¶æ€
func (ut *UsageTracker) RecordFailedRequestTokens(requestID, modelName string, tokens *TokenUsage, duration time.Duration, failureReason string) {
	if ut.config == nil || !ut.config.Enabled || tokens == nil {
		return
	}

	// åˆ›å»ºç‰¹æ®Šçš„å¤±è´¥è¯·æ±‚å®Œæˆäº‹ä»¶
	event := RequestEvent{
		Type:      "failed_request_tokens", // æ–°çš„äº‹ä»¶ç±»å‹
		RequestID: requestID,
		Timestamp: time.Now(),
		Data: RequestCompleteData{
			ModelName:           modelName,
			InputTokens:         tokens.InputTokens,
			OutputTokens:        tokens.OutputTokens,
			CacheCreationTokens: tokens.CacheCreationTokens,
			CacheReadTokens:     tokens.CacheReadTokens,
			Duration:            duration,
			FailureReason:       failureReason, // æ–°å¢å¤±è´¥åŸå› å­—æ®µ
		},
	}

	select {
	case ut.eventChan <- event:
		slog.Debug(fmt.Sprintf("ğŸ’¾ [å¤±è´¥Tokenäº‹ä»¶] [%s] åŸå› : %s, æ¨¡å‹: %s", requestID, failureReason, modelName))
	default:
		slog.Warn("Usage tracking event buffer full, dropping failed request tokens event",
			"request_id", requestID)
	}
}

// UpdatePricing æ›´æ–°æ¨¡å‹å®šä»·ï¼ˆè¿è¡Œæ—¶åŠ¨æ€æ›´æ–°ï¼‰
func (ut *UsageTracker) UpdatePricing(pricing map[string]ModelPricing) {
	ut.mu.Lock()
	defer ut.mu.Unlock()
	
	ut.pricing = pricing
	slog.Info("Model pricing updated", "model_count", len(pricing))
}

// GetDatabaseStats è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…è£…æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetDatabaseStats(ctx context.Context) (*DatabaseStats, error) {
	if ut.config == nil || !ut.config.Enabled {
		return nil, fmt.Errorf("usage tracking not enabled")
	}
	if ut.db == nil {
		return nil, fmt.Errorf("database not initialized")
	}
	return ut.getDatabaseStatsInternal(ctx)
}

// HealthCheck æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬åŠŸèƒ½ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) HealthCheck(ctx context.Context) error {
	if ut.config == nil || !ut.config.Enabled {
		return nil // å¦‚æœæœªå¯ç”¨ï¼Œè®¤ä¸ºæ˜¯å¥åº·çš„
	}
	
	if ut.readDB == nil {
		return fmt.Errorf("read database not initialized")
	}
	
	// æµ‹è¯•è¯»æ•°æ®åº“è¿æ¥
	if err := ut.readDB.PingContext(ctx); err != nil {
		return fmt.Errorf("read database ping failed: %w", err)
	}
	
	// æµ‹è¯•å†™æ•°æ®åº“è¿æ¥
	if ut.writeDB != nil {
		if err := ut.writeDB.PingContext(ctx); err != nil {
			return fmt.Errorf("write database ping failed: %w", err)
		}
	}
	
	// æµ‹è¯•åŸºæœ¬æŸ¥è¯¢ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	var count int
	err := ut.readDB.QueryRowContext(ctx, "SELECT COUNT(*) FROM sqlite_master WHERE type='table'").Scan(&count)
	if err != nil {
		return fmt.Errorf("database query test failed: %w", err)
	}
	
	// æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
	if count < 2 { // è‡³å°‘åº”è¯¥æœ‰ request_logs å’Œ usage_summary ä¸¤ä¸ªè¡¨
		return fmt.Errorf("database schema incomplete: expected at least 2 tables, found %d", count)
	}
	
	// æ£€æŸ¥äº‹ä»¶å¤„ç†é€šé“æ˜¯å¦æ­£å¸¸
	select {
	case <-ut.ctx.Done():
		return fmt.Errorf("usage tracker context cancelled")
	default:
		// ä¸Šä¸‹æ–‡æ­£å¸¸
	}
	
	// æ£€æŸ¥äº‹ä»¶é€šé“å®¹é‡
	if ut.eventChan != nil {
		channelLoad := float64(len(ut.eventChan)) / float64(cap(ut.eventChan)) * 100
		if channelLoad > 90 {
			return fmt.Errorf("event channel overloaded: %.1f%% capacity used", channelLoad)
		}
	}
	
	// æ£€æŸ¥å†™é˜Ÿåˆ—å®¹é‡
	if ut.writeQueue != nil {
		writeQueueLoad := float64(len(ut.writeQueue)) / float64(cap(ut.writeQueue)) * 100
		if writeQueueLoad > 90 {
			return fmt.Errorf("write queue overloaded: %.1f%% capacity used", writeQueueLoad)
		}
	}
	
	return nil
}

// ForceFlush å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰å¾…å¤„ç†äº‹ä»¶
func (ut *UsageTracker) ForceFlush() error {
	if ut.config == nil || !ut.config.Enabled {
		return nil
	}
	
	// å°è¯•å‘é€ä¸€ä¸ªç‰¹æ®Šäº‹ä»¶æ¥è§¦å‘æ‰¹å¤„ç†
	flushEvent := RequestEvent{
		Type:      "flush",
		RequestID: "force-flush-" + time.Now().Format("20060102150405"),
		Timestamp: time.Now(),
		Data:      nil,
	}
	
	select {
	case ut.eventChan <- flushEvent:
		slog.Info("Force flush event sent")
		return nil
	default:
		return fmt.Errorf("event channel full, cannot force flush")
	}
}

// GetPricing è·å–æ¨¡å‹å®šä»·
func (ut *UsageTracker) GetPricing(modelName string) ModelPricing {
	ut.mu.RLock()
	defer ut.mu.RUnlock()
	
	if pricing, exists := ut.pricing[modelName]; exists {
		return pricing
	}
	return ut.config.DefaultPricing
}

// GetConfiguredModels è·å–é…ç½®ä¸­çš„æ‰€æœ‰æ¨¡å‹åˆ—è¡¨
func (ut *UsageTracker) GetConfiguredModels() []string {
	ut.mu.RLock()
	defer ut.mu.RUnlock()
	
	models := make([]string, 0, len(ut.pricing))
	for modelName := range ut.pricing {
		models = append(models, modelName)
	}
	
	return models
}

// GetUsageSummary è·å–ä½¿ç”¨æ‘˜è¦ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetUsageSummary(ctx context.Context, startTime, endTime time.Time) ([]UsageSummary, error) {
	opts := &QueryOptions{
		StartDate: &startTime,
		EndDate:   &endTime,
		Limit:     100,
	}
	return ut.QueryUsageSummary(ctx, opts)
}

// GetRequestLogs è·å–è¯·æ±‚æ—¥å¿—ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼‰
func (ut *UsageTracker) GetRequestLogs(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string, limit, offset int) ([]RequestDetail, error) {
	opts := &QueryOptions{
		StartDate:    &startTime,
		EndDate:      &endTime,
		ModelName:    modelName,
		EndpointName: endpointName,
		GroupName:    groupName,
		Limit:        limit,
		Offset:       offset,
	}
	return ut.QueryRequestDetails(ctx, opts)
}

// GetUsageStats è·å–ä½¿ç”¨ç»Ÿè®¡ï¼ˆä¾¿åˆ©æ–¹æ³•ï¼Œä½¿ç”¨è¯»è¿æ¥ï¼‰
func (ut *UsageTracker) GetUsageStats(ctx context.Context, startTime, endTime time.Time) (*UsageStatsDetailed, error) {
	if ut.readDB == nil {
		return nil, fmt.Errorf("read database not initialized")
	}

	query := `SELECT 
		COUNT(*) as total_requests,
		SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as success_requests,
		SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as error_requests,
		SUM(input_tokens + output_tokens + cache_creation_tokens + cache_read_tokens) as total_tokens,
		SUM(total_cost_usd) as total_cost
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ?`
	
	var stats UsageStatsDetailed
	err := ut.readDB.QueryRowContext(ctx, query, startTime, endTime).Scan(
		&stats.TotalRequests,
		&stats.SuccessRequests,
		&stats.ErrorRequests,
		&stats.TotalTokens,
		&stats.TotalCost,
	)
	if err != nil {
		return nil, fmt.Errorf("failed to query detailed usage stats: %w", err)
	}
	
	// è·å–æ¨¡å‹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	modelQuery := `SELECT model_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND model_name IS NOT NULL AND model_name != ''
		GROUP BY model_name`
	
	rows, err := ut.readDB.QueryContext(ctx, modelQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query model stats: %w", err)
	}
	defer rows.Close()
	
	stats.ModelStats = make(map[string]ModelStat)
	for rows.Next() {
		var modelName string
		var requests int64
		var cost float64
		if err := rows.Scan(&modelName, &requests, &cost); err != nil {
			continue
		}
		stats.ModelStats[modelName] = ModelStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	// è·å–ç«¯ç‚¹ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	endpointQuery := `SELECT endpoint_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND endpoint_name IS NOT NULL AND endpoint_name != ''
		GROUP BY endpoint_name`
	
	rows2, err := ut.readDB.QueryContext(ctx, endpointQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query endpoint stats: %w", err)
	}
	defer rows2.Close()
	
	stats.EndpointStats = make(map[string]EndpointStat)
	for rows2.Next() {
		var endpointName string
		var requests int64
		var cost float64
		if err := rows2.Scan(&endpointName, &requests, &cost); err != nil {
			continue
		}
		stats.EndpointStats[endpointName] = EndpointStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	// è·å–ç»„ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯»è¿æ¥ï¼‰
	groupQuery := `SELECT group_name, COUNT(*), SUM(total_cost_usd)
		FROM request_logs 
		WHERE start_time >= ? AND start_time <= ? AND group_name IS NOT NULL AND group_name != ''
		GROUP BY group_name`
	
	rows3, err := ut.readDB.QueryContext(ctx, groupQuery, startTime, endTime)
	if err != nil {
		return nil, fmt.Errorf("failed to query group stats: %w", err)
	}
	defer rows3.Close()
	
	stats.GroupStats = make(map[string]GroupStat)
	for rows3.Next() {
		var groupName string
		var requests int64
		var cost float64
		if err := rows3.Scan(&groupName, &requests, &cost); err != nil {
			continue
		}
		stats.GroupStats[groupName] = GroupStat{
			RequestCount: requests,
			TotalCost:    cost,
		}
	}
	
	return &stats, nil
}

// ExportToCSV å¯¼å‡ºä¸ºCSVæ ¼å¼
func (ut *UsageTracker) ExportToCSV(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for CSV export: %w", err)
	}
	
	// CSV header
	csv := "request_id,client_ip,user_agent,method,path,start_time,end_time,duration_ms,endpoint_name,group_name,model_name,status,http_status_code,retry_count,input_tokens,output_tokens,cache_creation_tokens,cache_read_tokens,input_cost_usd,output_cost_usd,cache_creation_cost_usd,cache_read_cost_usd,total_cost_usd,created_at,updated_at\n"
	
	// CSV rows
	for _, log := range logs {
		endTime := ""
		if log.EndTime != nil {
			endTime = log.EndTime.Format(time.RFC3339)
		}
		
		durationMs := ""
		if log.DurationMs != nil {
			durationMs = fmt.Sprintf("%d", *log.DurationMs)
		}
		
		httpStatus := ""
		if log.HTTPStatusCode != nil {
			httpStatus = fmt.Sprintf("%d", *log.HTTPStatusCode)
		}
		
		csv += fmt.Sprintf("%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%d,%d,%d,%d,%d,%.6f,%.6f,%.6f,%.6f,%.6f,%s,%s\n",
			log.RequestID, log.ClientIP, log.UserAgent, log.Method, log.Path,
			log.StartTime.Format(time.RFC3339), endTime, durationMs,
			log.EndpointName, log.GroupName, log.ModelName, log.Status,
			httpStatus, log.RetryCount,
			log.InputTokens, log.OutputTokens, log.CacheCreationTokens, log.CacheReadTokens,
			log.InputCostUSD, log.OutputCostUSD, log.CacheCreationCostUSD, log.CacheReadCostUSD, log.TotalCostUSD,
			log.CreatedAt.Format(time.RFC3339), log.UpdatedAt.Format(time.RFC3339),
		)
	}
	
	return []byte(csv), nil
}

// ExportToJSON å¯¼å‡ºä¸ºJSONæ ¼å¼
func (ut *UsageTracker) ExportToJSON(ctx context.Context, startTime, endTime time.Time, modelName, endpointName, groupName string) ([]byte, error) {
	logs, err := ut.GetRequestLogs(ctx, startTime, endTime, modelName, endpointName, groupName, 10000, 0) // Export up to 10k records
	if err != nil {
		return nil, fmt.Errorf("failed to get request logs for JSON export: %w", err)
	}
	
	// ä½¿ç”¨æ ‡å‡†åº“çš„jsonåŒ…åºåˆ—åŒ–
	jsonBytes, err := json.Marshal(logs)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal logs to JSON: %w", err)
	}
	
	return jsonBytes, nil
}

// startWriteProcessor å¯åŠ¨å†™æ“ä½œé˜Ÿåˆ—å¤„ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼Œç¡®ä¿ç¨³å®šæ€§ï¼‰
func (ut *UsageTracker) startWriteProcessor() {
	ut.writeWg.Add(1)
	go func() {
		defer ut.writeWg.Done()
		slog.Debug("Write processor started")
		
		for {
			select {
			case writeReq := <-ut.writeQueue:
				err := ut.executeWriteSimple(writeReq)
				writeReq.Response <- err
				
			case <-ut.ctx.Done():
				slog.Debug("Write processor stopped")
				return
			}
		}
	}()
}

// executeWriteSimple æ‰§è¡Œç®€å•å†™æ“ä½œï¼ˆé¿å…å¤æ‚çš„æ‰¹å¤„ç†ï¼‰
func (ut *UsageTracker) executeWriteSimple(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()
	
	ctx, cancel := context.WithTimeout(req.Context, 30*time.Second)
	defer cancel()
	
	// ç›´æ¥æ‰§è¡Œï¼Œä¸ä½¿ç”¨äº‹åŠ¡ï¼ˆå¯¹äºç®€å•INSERT/UPDATEï¼Œä¸ä¸€å®šéœ€è¦äº‹åŠ¡ï¼‰
	if req.EventType == "vacuum" {
		// VACUUMä¸èƒ½åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œ
		_, err := ut.writeDB.ExecContext(ctx, req.Query, req.Args...)
		return err
	}
	
	// å…¶ä»–æ“ä½œä½¿ç”¨çŸ­äº‹åŠ¡
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	
	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Debug("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()
	
	_, err = tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}
	
	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}
	
	committed = true
	return nil
}

// executeWrite æ‰§è¡Œå•ä¸ªå†™æ“ä½œ
func (ut *UsageTracker) executeWrite(req WriteRequest) error {
	ut.writeMu.Lock()
	defer ut.writeMu.Unlock()
	
	ctx, cancel := context.WithTimeout(req.Context, 60*time.Second)
	defer cancel()
	
	// å®‰å…¨çš„äº‹åŠ¡å¤„ç†
	return ut.executeWriteTransaction(ctx, req)
}

// executeWriteTransaction æ‰§è¡Œå†™äº‹åŠ¡ï¼ˆä¿®å¤defer tx.Rollback()é—®é¢˜ï¼‰
func (ut *UsageTracker) executeWriteTransaction(ctx context.Context, req WriteRequest) error {
	tx, err := ut.writeDB.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("failed to begin transaction: %w", err)
	}
	
	committed := false
	defer func() {
		if !committed {
			if rbErr := tx.Rollback(); rbErr != nil {
				slog.Error("Failed to rollback transaction", "error", rbErr, "event_type", req.EventType)
			}
		}
	}()
	
	_, err = tx.ExecContext(ctx, req.Query, req.Args...)
	if err != nil {
		return fmt.Errorf("failed to execute query: %w", err)
	}
	
	err = tx.Commit()
	if err != nil {
		return fmt.Errorf("failed to commit transaction: %w", err)
	}
	
	committed = true  // æ ‡è®°å·²æäº¤ï¼Œé¿å…é‡å¤Rollback
	return nil
}

// initDatabaseWithWriteDB ä½¿ç”¨å†™è¿æ¥åˆå§‹åŒ–æ•°æ®åº“ï¼ˆåŒæ­¥ç­‰å¾…å®Œæˆï¼‰
func (ut *UsageTracker) initDatabaseWithWriteDB() error {
	// è¯»å–å¹¶æ‰§è¡Œ schema SQL
	schemaSQL, err := schemaFS.ReadFile("schema.sql")
	if err != nil {
		return fmt.Errorf("failed to read schema.sql: %w", err)
	}

	// ä½¿ç”¨å†™è¿æ¥ç›´æ¥æ‰§è¡Œ schemaï¼ˆåŒæ­¥æ–¹å¼ï¼Œç¡®ä¿è¡¨åˆ›å»ºå®Œæˆï¼‰
	if _, err := ut.writeDB.Exec(string(schemaSQL)); err != nil {
		return fmt.Errorf("failed to execute schema: %w", err)
	}

	slog.Debug("Database schema initialized successfully with write connection")
	return nil
}